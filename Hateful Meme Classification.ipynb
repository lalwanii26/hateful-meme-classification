{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58d6cc8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'img', 'text'], dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# f = pd.read_json(file_path, lines=True)\n",
    "df_image_train = pd.read_json(\"data/train.jsonl\", lines=True)\n",
    "df_image_val = pd.read_json(\"data/dev.jsonl\", lines=True)\n",
    "df_image_test = pd.read_json(\"data/test.jsonl\", lines=True)\n",
    "# df_image_val.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc7e9005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.9/site-packages (1.9.0+cu111)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch) (4.5.0)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/musharma/.local/lib/python3.9/site-packages (4.29.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers) (2023.5.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers) (4.61.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/musharma/.local/lib/python3.9/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: filelock in /home/musharma/.local/lib/python3.9/site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/musharma/.local/lib/python3.9/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2021.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (2.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (2023.5.7)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence-transformers in /home/musharma/.local/lib/python3.9/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from sentence-transformers) (1.22.4)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.9/site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /home/musharma/.local/lib/python3.9/site-packages (from sentence-transformers) (0.15.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /home/musharma/.local/lib/python3.9/site-packages (from sentence-transformers) (4.29.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from sentence-transformers) (4.61.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from sentence-transformers) (1.7.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.9/site-packages (from sentence-transformers) (0.10.0+cu111)\n",
      "Requirement already satisfied: sentencepiece in /home/musharma/.local/lib/python3.9/site-packages (from sentence-transformers) (0.1.99)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.9/site-packages (from sentence-transformers) (0.24.2)\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from sentence-transformers) (1.9.0+cu111)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (5.4.1)\n",
      "Requirement already satisfied: filelock in /home/musharma/.local/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2021.7.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.26.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (2.4.7)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/musharma/.local/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.5.5)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->sentence-transformers) (1.0.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->sentence-transformers) (8.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.6)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /opt/conda/lib/python3.9/site-packages (from torchvision->sentence-transformers) (8.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dc19b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: This framework generates embeddings for each input sentence\n",
      "Embedding: [-1.37173524e-02 -4.28515933e-02 -1.56286098e-02  1.40537601e-02\n",
      "  3.95537615e-02  1.21796273e-01  2.94333510e-02 -3.17523740e-02\n",
      "  3.54959592e-02 -7.93140307e-02  1.75878126e-02 -4.04369533e-02\n",
      "  4.97259796e-02  2.54913010e-02 -7.18699992e-02  8.14968497e-02\n",
      "  1.47073215e-03  4.79627401e-02 -4.50335406e-02 -9.92174894e-02\n",
      " -2.81769633e-02  6.45045787e-02  4.44670655e-02 -4.76217344e-02\n",
      " -3.52952220e-02  4.38671820e-02 -5.28566204e-02  4.33004228e-04\n",
      "  1.01921462e-01  1.64072476e-02  3.26996781e-02 -3.45987007e-02\n",
      "  1.21339587e-02  7.94871375e-02  4.58342675e-03  1.57778431e-02\n",
      " -9.68208257e-03  2.87626348e-02 -5.05806394e-02 -1.55793848e-02\n",
      " -2.87907179e-02 -9.62278340e-03  3.15556489e-02  2.27349754e-02\n",
      "  8.71449485e-02 -3.85027528e-02 -8.84718522e-02 -8.75496119e-03\n",
      " -2.12343484e-02  2.08924413e-02 -9.02078077e-02 -5.25732227e-02\n",
      " -1.05638560e-02  2.88311280e-02 -1.61455050e-02  6.17836602e-03\n",
      " -1.23234382e-02 -1.07336985e-02  2.83353552e-02 -5.28567694e-02\n",
      " -3.58617865e-02 -5.97989410e-02 -1.09054968e-02  2.91566513e-02\n",
      "  7.97978938e-02 -3.27859510e-04  6.83497265e-03  1.32718794e-02\n",
      " -4.24619727e-02  1.87656879e-02 -9.89234671e-02  2.09050365e-02\n",
      " -8.69605914e-02 -1.50152026e-02 -4.86202538e-02  8.04414526e-02\n",
      " -3.67699703e-03 -6.65044412e-02  1.14556782e-01 -3.04228682e-02\n",
      "  2.96631586e-02 -2.80695092e-02  4.64990400e-02 -2.25513931e-02\n",
      "  8.54223073e-02  3.15446816e-02  7.34542161e-02 -2.21861713e-02\n",
      " -5.29679433e-02  1.27130458e-02 -5.27339913e-02 -1.06188744e-01\n",
      "  7.04731643e-02  2.76736282e-02 -8.05531070e-02  2.39649564e-02\n",
      " -2.65124999e-02 -2.17331108e-02  4.35275361e-02  4.84711789e-02\n",
      " -2.37067007e-02  2.85768434e-02  1.11846097e-01 -6.34936020e-02\n",
      " -1.58318523e-02 -2.26169527e-02 -1.31028332e-02 -1.62068987e-03\n",
      " -3.60928997e-02 -9.78297070e-02 -4.67729568e-02  1.76272113e-02\n",
      " -3.97491939e-02 -1.76401009e-04  3.39627974e-02 -2.09633764e-02\n",
      "  6.33663451e-03 -2.59411503e-02  8.10410827e-02  6.14393242e-02\n",
      " -5.44600235e-03  6.48275986e-02 -1.16844043e-01  2.36861203e-02\n",
      " -1.32058831e-02 -1.12476446e-01  1.90049317e-02 -1.74660541e-34\n",
      "  5.58949299e-02  1.94244459e-02  4.65438478e-02  5.18645681e-02\n",
      "  3.89390215e-02  3.40540707e-02 -4.32114340e-02  7.90637434e-02\n",
      " -9.79530215e-02 -1.27441119e-02 -2.91871168e-02  1.02052381e-02\n",
      "  1.88115966e-02  1.08942531e-01  6.63465112e-02 -5.35295308e-02\n",
      " -3.29229012e-02  4.69826758e-02  2.28883084e-02  2.74114460e-02\n",
      " -2.91983541e-02  3.12706642e-02 -2.22850628e-02 -1.02282129e-01\n",
      " -2.79116668e-02  1.13792745e-02  9.06309187e-02 -4.75414731e-02\n",
      " -1.00718908e-01 -1.23232147e-02 -7.96928629e-02 -1.44636929e-02\n",
      " -7.76400715e-02 -7.66918855e-03  9.73956194e-03  2.24204659e-02\n",
      "  7.77267739e-02 -3.17156222e-03  2.11538505e-02 -3.30394134e-02\n",
      "  9.55249369e-03 -3.73012014e-02  2.61360426e-02 -9.79083776e-03\n",
      " -6.31505027e-02  5.77434478e-03 -3.80031429e-02  1.29684135e-02\n",
      " -1.82499178e-02 -1.56283043e-02 -1.23360485e-03  5.55579029e-02\n",
      "  1.13105096e-04 -5.61257005e-02  7.40165561e-02  1.84451863e-02\n",
      " -2.66368035e-02  1.31951757e-02  7.50086755e-02 -2.46797577e-02\n",
      " -3.24006081e-02 -1.57674812e-02 -8.03513825e-03 -5.61321387e-03\n",
      "  1.05687603e-02  3.26166605e-03 -3.91989946e-02 -9.38676894e-02\n",
      "  1.14227116e-01  6.57304898e-02 -4.72633280e-02  1.45088071e-02\n",
      " -3.54490392e-02 -3.37761566e-02 -5.15505970e-02 -3.80997360e-03\n",
      " -5.15036210e-02 -5.93429767e-02 -1.69412384e-03  7.42107481e-02\n",
      " -4.20091301e-02 -7.19975382e-02  3.17249894e-02 -1.66303683e-02\n",
      "  3.96983419e-03 -6.52751029e-02  2.77391244e-02 -7.51649886e-02\n",
      "  2.27456354e-02 -3.91368270e-02  1.54315922e-02 -5.54908626e-02\n",
      "  1.23318471e-02 -2.59520430e-02  6.66423440e-02 -6.91258315e-34\n",
      "  3.31628919e-02  8.47928971e-02 -6.65584132e-02  3.33541296e-02\n",
      "  4.71607409e-03  1.35361878e-02 -5.38694188e-02  9.20693725e-02\n",
      " -2.96876784e-02  3.16219665e-02 -2.37497613e-02  1.98771376e-02\n",
      "  1.03446200e-01 -9.06947255e-02  6.30628970e-03  1.42886192e-02\n",
      "  1.19293705e-02  6.43732585e-03  4.20104489e-02  1.25344452e-02\n",
      "  3.93019244e-02  5.35691567e-02 -4.30749916e-02  6.10433072e-02\n",
      " -5.39267712e-05  6.91682994e-02  1.05520533e-02  1.22111430e-02\n",
      " -7.23185688e-02  2.50469074e-02 -5.18371016e-02 -4.36562113e-02\n",
      " -6.71818182e-02  1.34828174e-02 -7.25888759e-02  7.04162754e-03\n",
      "  6.58939108e-02  1.08994069e-02 -2.60010781e-03  5.49968593e-02\n",
      "  5.06966822e-02  3.27948667e-02 -6.68832958e-02  6.45557046e-02\n",
      " -2.52076257e-02 -2.92571858e-02 -1.16696715e-01  3.24064158e-02\n",
      "  5.85858636e-02 -3.51756439e-02 -7.15240091e-02  2.24936008e-02\n",
      " -1.00786708e-01 -4.74544838e-02 -7.61962533e-02 -5.87166436e-02\n",
      "  4.21138294e-02 -7.47214034e-02  1.98468063e-02 -3.36505217e-03\n",
      " -5.29736690e-02  2.74729151e-02  3.45736928e-02 -6.11847080e-02\n",
      "  1.06364764e-01 -9.64120179e-02 -4.55944836e-02  1.51490122e-02\n",
      " -5.13531035e-03 -6.64447397e-02  4.31721546e-02 -1.10405562e-02\n",
      " -9.80251376e-03  7.53783360e-02 -1.49570750e-02 -4.80208397e-02\n",
      "  5.80726489e-02 -2.43896693e-02 -2.23138407e-02 -4.36992757e-02\n",
      "  5.12054190e-02 -3.28625813e-02  1.08763337e-01  6.08925931e-02\n",
      "  3.30794579e-03  5.53819910e-02  8.43200684e-02  1.27087403e-02\n",
      "  3.84465642e-02  6.52325749e-02 -2.94684079e-02  5.08005284e-02\n",
      " -2.09348183e-02  1.46135673e-01  2.25561745e-02 -1.77227744e-08\n",
      " -5.02673015e-02 -2.79193133e-04 -1.00328557e-01  2.42811404e-02\n",
      " -7.54043311e-02 -3.79139856e-02  3.96049693e-02  3.10079772e-02\n",
      " -9.05704033e-03 -6.50411919e-02  4.05453257e-02  4.83390205e-02\n",
      " -4.56962213e-02  4.76002647e-03  2.64363014e-03  9.35614184e-02\n",
      " -4.02598977e-02  3.27401571e-02  1.18298829e-02  5.54345064e-02\n",
      "  1.48052260e-01  7.21189603e-02  2.76975421e-04  1.68651193e-02\n",
      "  8.34879000e-03 -8.76155961e-03 -1.33650051e-02  6.14236593e-02\n",
      "  1.57168023e-02  6.94960877e-02  1.08621679e-02  6.08018413e-02\n",
      " -5.33421114e-02 -3.47925052e-02 -3.36272120e-02  6.93906993e-02\n",
      "  1.22987907e-02 -1.45237356e-01 -2.06971262e-03 -4.61132973e-02\n",
      "  3.72750056e-03 -5.59354527e-03 -1.00659862e-01 -4.45952974e-02\n",
      "  5.40921092e-02  4.98894323e-03  1.49534512e-02 -8.26059356e-02\n",
      "  6.26630709e-02 -5.01909340e-03 -4.81857546e-02 -3.53991203e-02\n",
      "  9.03387275e-03 -2.42337361e-02  5.66267483e-02  2.51528937e-02\n",
      " -1.70709658e-02 -1.24780042e-02  3.19517963e-02  1.38420854e-02\n",
      " -1.55814718e-02  1.00178294e-01  1.23657271e-01 -4.22967002e-02]\n",
      "\n",
      "Sentence: Sentences are passed as a list of string.\n",
      "Embedding: [ 5.64524569e-02  5.50023876e-02  3.13795581e-02  3.39485109e-02\n",
      " -3.54247019e-02  8.34667459e-02  9.88800824e-02  7.27545330e-03\n",
      " -6.68661110e-03 -7.65808672e-03  7.93738365e-02  7.39684911e-04\n",
      "  1.49291977e-02 -1.51047027e-02  3.67674418e-02  4.78742868e-02\n",
      " -4.81969416e-02 -3.76051962e-02 -4.60277982e-02 -8.89815986e-02\n",
      "  1.20228149e-01  1.30663291e-01 -3.73936370e-02  2.47855764e-03\n",
      "  2.55825114e-03  7.25814477e-02 -6.80436417e-02 -5.24696037e-02\n",
      "  4.90234233e-02  2.99563278e-02 -5.84429428e-02 -2.02263054e-02\n",
      "  2.08822060e-02  9.76691842e-02  3.52390558e-02  3.91141325e-02\n",
      "  1.05668101e-02  1.56231225e-03 -1.30822835e-02  8.52904934e-03\n",
      " -4.84094629e-03 -2.03766599e-02 -2.71800924e-02  2.83307694e-02\n",
      "  3.66017707e-02  2.51276474e-02 -9.90861803e-02  1.15626371e-02\n",
      " -3.60380523e-02 -7.23784193e-02 -1.12670109e-01  1.12942411e-02\n",
      " -3.86397503e-02  4.67386171e-02 -2.88460702e-02  2.26703733e-02\n",
      " -8.52403790e-03  3.32815163e-02 -1.06582534e-03 -7.09744766e-02\n",
      " -6.31169975e-02 -5.72186746e-02 -6.16026409e-02  5.47146536e-02\n",
      "  1.18317856e-02 -4.66261171e-02  2.56960094e-02 -7.07412837e-03\n",
      " -5.73842861e-02  4.12839316e-02 -5.91503717e-02  5.89021742e-02\n",
      " -4.41697575e-02  4.65081409e-02 -3.15814316e-02  5.58312312e-02\n",
      "  5.54578975e-02 -5.96533194e-02  4.06407118e-02  4.83764568e-03\n",
      " -4.96768467e-02 -1.00944340e-01  3.40078473e-02  4.13273508e-03\n",
      " -2.93529569e-03  2.11837776e-02 -3.73962224e-02 -2.79066954e-02\n",
      " -4.61767539e-02  5.26138805e-02 -2.79734805e-02 -1.62379280e-01\n",
      "  6.61042854e-02  1.72274448e-02 -5.45111252e-03  4.74474095e-02\n",
      " -3.82237360e-02 -3.96896787e-02  1.34544652e-02  4.49653678e-02\n",
      "  4.53669578e-03  2.82978714e-02  8.36632997e-02 -1.00857848e-02\n",
      " -1.19353987e-01 -3.84624489e-02  4.82858755e-02 -9.46083590e-02\n",
      "  1.91854183e-02 -9.96518731e-02 -6.30596876e-02  3.02696358e-02\n",
      "  1.17402319e-02 -4.78372872e-02 -6.20271033e-03 -3.32850814e-02\n",
      " -4.04391019e-03  1.28307221e-02  4.05254774e-02  7.56476820e-02\n",
      "  2.92434897e-02  2.84270309e-02 -2.78938375e-02  1.66857950e-02\n",
      " -2.47961506e-02 -6.83651119e-02  2.89967936e-02 -5.39867748e-33\n",
      " -2.69015227e-03 -2.65069026e-02 -6.47903420e-04 -8.46199784e-03\n",
      " -7.35154748e-02  4.94083809e-03 -5.97842112e-02  1.03438236e-02\n",
      "  2.12904019e-03 -2.88212509e-03 -3.17076333e-02 -9.42363963e-02\n",
      "  3.03019639e-02  7.00226948e-02  4.50685285e-02  3.69439460e-02\n",
      "  1.13593666e-02  3.53027321e-02  5.50447451e-03  1.34414842e-03\n",
      "  3.46122868e-03  7.75047839e-02  5.45112491e-02 -7.92055503e-02\n",
      " -9.31696445e-02 -4.03398313e-02  3.10668852e-02 -3.83081660e-02\n",
      " -5.89442514e-02  1.93331931e-02 -2.67159995e-02 -7.91938305e-02\n",
      "  1.04222883e-04  7.70621151e-02  4.16603647e-02  8.90932530e-02\n",
      "  3.56843323e-02 -1.09152924e-02  3.71498577e-02 -2.07070522e-02\n",
      " -2.46100575e-02 -2.05025636e-02  2.62201540e-02  3.43590267e-02\n",
      "  4.39250916e-02 -8.20518378e-03 -8.40710625e-02  4.24170978e-02\n",
      "  4.87498678e-02  5.95384911e-02  2.87747663e-02  3.37638333e-02\n",
      " -4.07442711e-02 -1.66369730e-03  7.91927502e-02  3.41088250e-02\n",
      " -5.72834455e-04  1.87749732e-02 -1.36964209e-02  7.38333240e-02\n",
      "  5.74461767e-04  8.33505318e-02  5.60810715e-02 -1.13711208e-02\n",
      "  4.42611240e-02  2.69581825e-02 -4.80535962e-02 -3.15087289e-02\n",
      "  7.75226131e-02  1.81773286e-02 -8.83005187e-02 -7.85518065e-03\n",
      " -6.22242987e-02  7.19372928e-02 -2.33475007e-02  6.52482640e-03\n",
      " -9.49527510e-03 -9.88312736e-02  4.01306152e-02  3.07396706e-02\n",
      " -2.21607070e-02 -9.45911184e-02  1.02367960e-02  1.02187790e-01\n",
      " -4.12959866e-02 -3.15777808e-02  4.74752001e-02 -1.10209815e-01\n",
      "  1.69614516e-02 -3.71709354e-02 -1.03262076e-02 -4.72538620e-02\n",
      " -1.20214215e-02 -1.93255041e-02  5.79292215e-02  4.23865401e-34\n",
      "  3.92013118e-02  8.41361359e-02 -1.02946721e-01  6.92259744e-02\n",
      "  1.68821085e-02 -3.26760933e-02  9.65958275e-03  1.80899687e-02\n",
      "  2.17939913e-02  1.63189061e-02 -9.69292223e-02  3.74853797e-03\n",
      " -2.38457303e-02 -3.44056003e-02  7.11962506e-02  9.21906671e-04\n",
      " -6.23862119e-03  3.23754288e-02 -8.90384661e-04  5.01906034e-03\n",
      " -4.24538180e-02  9.89083797e-02 -4.60320823e-02  4.69704792e-02\n",
      " -1.75284166e-02 -7.02517945e-03  1.32743884e-02 -5.30151986e-02\n",
      "  2.66404473e-03  1.45819429e-02  7.43346708e-03 -3.07131726e-02\n",
      " -2.09416579e-02  8.24110284e-02 -5.15894294e-02 -2.71178316e-02\n",
      "  1.17583014e-01  7.72506837e-03 -1.89523101e-02  3.94559503e-02\n",
      "  7.17360526e-02  2.59117242e-02  2.75191832e-02  9.50543582e-03\n",
      " -3.02355457e-02 -4.07944806e-02 -1.04028478e-01 -7.97418039e-03\n",
      " -3.64454533e-03  3.29716280e-02 -2.35954281e-02 -7.50518031e-03\n",
      " -5.82234003e-02 -3.17906253e-02 -4.18049097e-02  2.17453670e-02\n",
      " -6.67292103e-02 -4.89104241e-02  4.58517345e-03 -2.66046673e-02\n",
      " -1.12597041e-01  5.11167571e-02  5.48533909e-02 -6.69856817e-02\n",
      "  1.26766264e-01 -8.59487727e-02 -5.94231784e-02 -2.92189396e-03\n",
      " -1.14875669e-02 -1.26025841e-01 -3.48279905e-03 -9.12001878e-02\n",
      " -1.22933082e-01  1.33777019e-02 -4.75775301e-02 -6.57933503e-02\n",
      " -3.39409895e-02 -3.07108015e-02 -5.22033870e-02 -2.35463660e-02\n",
      "  5.90035245e-02 -3.85757722e-02  3.19701098e-02  4.05118652e-02\n",
      "  1.67077910e-02 -3.58281434e-02  1.45687973e-02  3.20138149e-02\n",
      " -1.34843774e-02  6.07819781e-02 -8.31401721e-03 -1.08105959e-02\n",
      "  4.69410606e-02  7.66134188e-02 -4.23400216e-02 -2.11963318e-08\n",
      " -7.25292861e-02 -4.20227535e-02 -6.12374321e-02  5.24666049e-02\n",
      " -1.42363608e-02  1.18487170e-02 -1.40789011e-02 -3.67530100e-02\n",
      " -4.44977209e-02 -1.15140742e-02  5.23316897e-02  2.96652149e-02\n",
      " -4.62780558e-02 -3.70892733e-02  1.89129934e-02  2.04307493e-02\n",
      " -2.24006046e-02 -1.48562724e-02 -1.79504268e-02  4.20007855e-02\n",
      "  1.40942400e-02 -2.83492655e-02 -1.16863012e-01  1.48956804e-02\n",
      " -7.30591710e-04  5.66028245e-02 -2.68739611e-02  1.09106682e-01\n",
      "  2.94567528e-03  1.19267881e-01  1.14212409e-01  8.92973617e-02\n",
      " -1.70255341e-02 -4.99054119e-02 -2.11930834e-02  3.18421498e-02\n",
      "  7.03435689e-02 -1.02929428e-01  8.23816732e-02  2.81968173e-02\n",
      "  3.21146511e-02  3.79107818e-02 -1.09553099e-01  8.19620267e-02\n",
      "  8.73216391e-02 -5.73563725e-02 -2.01709121e-02 -5.69443963e-02\n",
      " -1.30338585e-02 -5.55684455e-02 -1.32966489e-02  8.64012819e-03\n",
      "  5.30012213e-02 -4.06847037e-02  2.71709263e-02 -2.55945162e-03\n",
      "  3.05775404e-02 -4.61865403e-02  4.68033459e-03 -3.64946909e-02\n",
      "  6.80802613e-02  6.65087476e-02  8.49152058e-02 -3.32848877e-02]\n",
      "\n",
      "Sentence: The quick brown fox jumps over the lazy dog.\n",
      "Embedding: [ 4.39335629e-02  5.89343533e-02  4.81783859e-02  7.75481090e-02\n",
      "  2.67444365e-02 -3.76296043e-02 -2.60508643e-03 -5.99430650e-02\n",
      " -2.49600806e-03  2.20728293e-02  4.80259955e-02  5.57552911e-02\n",
      " -3.89454328e-02 -2.66167913e-02  7.69340619e-03 -2.62376405e-02\n",
      " -3.64160575e-02 -3.78161371e-02  7.40781575e-02 -4.95050326e-02\n",
      " -5.85216880e-02 -6.36197031e-02  3.24350186e-02  2.20085215e-02\n",
      " -7.10637867e-02 -3.31577659e-02 -6.94103837e-02 -5.00374548e-02\n",
      "  7.46268257e-02 -1.11133806e-01 -1.23063065e-02  3.77456248e-02\n",
      " -2.80313008e-02  1.45353563e-02 -3.15585509e-02 -8.05836394e-02\n",
      "  5.83526045e-02  2.59012613e-03  3.92802469e-02  2.57695876e-02\n",
      "  4.98505756e-02 -1.75621943e-03 -4.55297753e-02  2.92607676e-02\n",
      " -1.02017254e-01  5.22287488e-02 -7.90899768e-02 -1.02857593e-02\n",
      "  9.20249522e-03  1.30732385e-02 -4.04777750e-02 -2.77925059e-02\n",
      "  1.24667641e-02  6.72832876e-02  6.81247711e-02 -7.57121202e-03\n",
      " -6.09946065e-03 -4.23776992e-02  5.17815985e-02 -1.56707466e-02\n",
      "  9.56357550e-03  4.12390307e-02  2.14959551e-02  1.04293413e-02\n",
      "  2.73349583e-02  1.87062509e-02 -2.69607231e-02 -7.00541809e-02\n",
      " -1.04700491e-01 -1.89877569e-03  1.77017320e-02 -5.74725270e-02\n",
      " -1.44222928e-02  4.70480212e-04  2.33228831e-03 -2.51920335e-02\n",
      "  4.93004285e-02 -5.09609804e-02  6.31983280e-02  1.49165168e-02\n",
      " -2.70766709e-02 -4.52875793e-02 -4.90594059e-02  3.74940820e-02\n",
      "  3.84579822e-02  1.56901276e-03  3.09922304e-02  2.01630220e-02\n",
      " -1.24363424e-02 -3.06720175e-02 -2.78819129e-02 -6.89182654e-02\n",
      " -5.13677597e-02  2.14795340e-02  1.15747200e-02  1.25408091e-03\n",
      "  1.88766029e-02 -4.42318730e-02 -4.49817367e-02 -3.41867888e-03\n",
      "  1.31131420e-02  2.00099796e-02  1.21099763e-01  2.31074877e-02\n",
      " -2.20159814e-02 -3.28847021e-02 -3.15508689e-03  1.17840078e-04\n",
      "  9.91498604e-02  1.65239032e-02 -4.69670352e-03 -1.45366723e-02\n",
      " -3.71076097e-03  9.65135992e-02  2.85907984e-02  2.13481840e-02\n",
      " -7.17645362e-02 -2.41142195e-02 -4.40940484e-02 -1.07346877e-01\n",
      "  6.79945499e-02  1.30466819e-01 -7.97029883e-02  6.79508736e-03\n",
      " -2.37511937e-02 -4.61636409e-02 -2.99650244e-02 -3.69410083e-33\n",
      "  7.30969831e-02 -2.20171791e-02 -8.61464664e-02 -7.14380145e-02\n",
      " -6.36741370e-02 -7.21863210e-02 -5.93041582e-03 -2.33641658e-02\n",
      " -2.83658430e-02  4.77434658e-02 -8.06176662e-02 -1.56481203e-03\n",
      "  1.38444146e-02 -2.86236014e-02 -3.35387029e-02 -1.13777578e-01\n",
      " -9.17637069e-03 -1.08101238e-02  3.23196426e-02  5.88380583e-02\n",
      "  3.34208943e-02  1.07987933e-01 -3.72712985e-02 -2.96770632e-02\n",
      "  5.17189838e-02 -2.25338694e-02 -6.96091279e-02 -2.14474984e-02\n",
      " -2.33410578e-02  4.82199825e-02 -3.58766280e-02 -4.68990766e-02\n",
      " -3.97873446e-02  1.10813268e-01 -1.43007524e-02 -1.18464492e-01\n",
      "  5.82914986e-02 -6.25889227e-02 -2.94040889e-02  6.03238232e-02\n",
      " -2.44415645e-03  1.60116274e-02  2.67233327e-02  2.49530412e-02\n",
      " -6.49319291e-02 -1.06802462e-02  2.81464495e-02  1.03563489e-02\n",
      " -6.63583283e-04  1.98186152e-02 -3.04288194e-02  6.28420850e-03\n",
      "  5.15268445e-02 -4.75375429e-02 -6.44421503e-02  9.55031887e-02\n",
      "  7.55858570e-02 -2.81574801e-02 -3.49965729e-02  1.01816416e-01\n",
      "  1.98732167e-02 -3.68036851e-02  2.93521513e-03 -5.00745289e-02\n",
      "  1.50932148e-01 -6.16079830e-02 -8.58812928e-02  7.13991653e-03\n",
      " -1.33065525e-02  7.80404583e-02  1.75250471e-02  4.21279222e-02\n",
      "  3.57939973e-02 -1.32950410e-01  3.56970094e-02 -2.03116685e-02\n",
      "  1.24910120e-02 -3.80355120e-02  4.91543561e-02 -1.56540964e-02\n",
      "  1.21418260e-01 -8.08644891e-02 -4.68781739e-02  4.10842970e-02\n",
      " -1.84318051e-02  6.69691488e-02  4.33592731e-03  2.27315426e-02\n",
      " -1.36429183e-02 -4.53238115e-02 -3.92829478e-02 -6.29893318e-03\n",
      "  5.29609658e-02 -3.69064994e-02  7.11677298e-02  2.33343343e-33\n",
      "  1.05231375e-01 -4.81874235e-02  6.95919022e-02  6.56976327e-02\n",
      " -4.65149656e-02  5.14492504e-02 -1.24475760e-02  3.20872106e-02\n",
      " -9.23356414e-02  5.00932373e-02 -3.28876376e-02  1.39139127e-02\n",
      " -8.70211341e-04 -4.90905391e-03  1.03946395e-01  3.21672211e-04\n",
      "  5.28110005e-02 -1.17990104e-02  2.31565628e-02  1.31767988e-02\n",
      " -5.25963344e-02  3.26702036e-02  3.08726012e-04  6.41129240e-02\n",
      "  3.88500951e-02  5.88008612e-02  8.29793587e-02 -1.88149698e-02\n",
      " -2.26376913e-02 -1.00473657e-01 -3.83752584e-02 -5.88081330e-02\n",
      "  1.82421063e-03 -4.26995121e-02  2.50195246e-02  6.40060082e-02\n",
      " -3.77483033e-02 -6.83904486e-03 -2.54606176e-03 -9.76042822e-02\n",
      "  1.88475978e-02 -8.83185130e-04  1.73611902e-02  7.10790455e-02\n",
      "  3.30393165e-02  6.93428470e-03 -5.60523383e-02  5.14634103e-02\n",
      " -4.29542549e-02  4.60077077e-02 -8.78832117e-03  3.17289196e-02\n",
      "  4.93965596e-02  2.95189656e-02 -5.05192243e-02 -5.43187372e-02\n",
      "  1.49925152e-04 -2.76614558e-02  3.46877761e-02 -2.10890472e-02\n",
      "  1.38060320e-02  2.99886614e-02  1.39744924e-02 -4.26470162e-03\n",
      " -1.50337322e-02 -8.76095369e-02 -6.85053766e-02 -4.28141803e-02\n",
      "  7.76945204e-02 -7.10285604e-02 -7.37693440e-03  2.13727597e-02\n",
      "  1.35562019e-02 -7.90464804e-02  5.47666056e-03  8.30664039e-02\n",
      "  1.14148043e-01  1.80763646e-03  8.75491127e-02 -4.16044854e-02\n",
      "  1.55416476e-02 -1.01206321e-02 -7.32438546e-03  1.07965674e-02\n",
      " -6.62816912e-02  3.98413800e-02 -1.16711549e-01  6.42994121e-02\n",
      "  4.02919874e-02 -6.54741302e-02  1.95052195e-02  8.09995905e-02\n",
      "  5.36463410e-02  7.67969713e-02 -1.34852426e-02 -1.76919084e-08\n",
      " -4.43935432e-02  9.20642540e-03 -8.79589841e-02  4.26921472e-02\n",
      "  7.31365234e-02  1.68427508e-02 -4.03262824e-02  1.85131039e-02\n",
      "  8.44172388e-02 -3.74477506e-02  3.02996319e-02  2.90641766e-02\n",
      "  6.36878908e-02  2.89749820e-02 -1.47269778e-02  1.77542549e-02\n",
      " -3.36895399e-02  1.73161719e-02  3.37875411e-02  1.76826075e-01\n",
      " -1.75533406e-02 -6.03077710e-02 -1.43394237e-02 -2.38536671e-02\n",
      " -4.45530787e-02 -2.89850514e-02 -8.96776244e-02 -1.75937009e-03\n",
      " -2.61486303e-02  5.93994418e-03 -5.18355221e-02  8.57279524e-02\n",
      " -8.18398967e-02  8.35440960e-03  4.00790013e-02  4.17764410e-02\n",
      "  1.04573563e-01 -2.86563602e-03  1.96690969e-02  5.81050338e-03\n",
      "  1.33253532e-02  4.51001376e-02 -2.17588469e-02 -1.39492983e-02\n",
      " -6.86992630e-02 -2.94107990e-03 -3.10765114e-02 -1.05854400e-01\n",
      "  6.91623613e-02 -4.24114466e-02 -4.67682481e-02 -3.64751071e-02\n",
      "  4.50399816e-02  6.09816648e-02 -6.56561777e-02 -5.45641128e-03\n",
      " -1.86227169e-02 -6.31484389e-02 -3.87436822e-02  3.46733704e-02\n",
      "  5.55458069e-02  5.21627963e-02  5.61065190e-02  1.02063976e-01]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "#Our sentences we like to encode\n",
    "sentences = ['This framework generates embeddings for each input sentence',\n",
    "    'Sentences are passed as a list of string.',\n",
    "    'The quick brown fox jumps over the lazy dog.']\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "#Print the embeddings\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9b88ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6876366138458252\n",
      "Epoch 2, Loss: 1.3108526468276978\n",
      "Epoch 3, Loss: 0.7335209250450134\n",
      "Epoch 4, Loss: 0.8349707722663879\n",
      "Epoch 5, Loss: 0.9584453701972961\n",
      "Epoch 6, Loss: 0.7120893001556396\n",
      "Epoch 7, Loss: 0.611566424369812\n",
      "Epoch 8, Loss: 0.7412387728691101\n",
      "Epoch 9, Loss: 0.7872076630592346\n",
      "Epoch 10, Loss: 0.6958101391792297\n",
      "Epoch 11, Loss: 0.6032505035400391\n",
      "Epoch 12, Loss: 0.6378868222236633\n",
      "Epoch 13, Loss: 0.6974227428436279\n",
      "Epoch 14, Loss: 0.6537891626358032\n",
      "Epoch 15, Loss: 0.5836477279663086\n",
      "Epoch 16, Loss: 0.5888790488243103\n",
      "Epoch 17, Loss: 0.6355252861976624\n",
      "Epoch 18, Loss: 0.6444496512413025\n",
      "Epoch 19, Loss: 0.6063341498374939\n",
      "Epoch 20, Loss: 0.5740800499916077\n",
      "Epoch 21, Loss: 0.587197482585907\n",
      "Epoch 22, Loss: 0.6090907454490662\n",
      "Epoch 23, Loss: 0.5930168032646179\n",
      "Epoch 24, Loss: 0.5661014318466187\n",
      "Epoch 25, Loss: 0.5713928937911987\n",
      "Epoch 26, Loss: 0.5897137522697449\n",
      "Epoch 27, Loss: 0.5849149227142334\n",
      "Epoch 28, Loss: 0.5637670755386353\n",
      "Epoch 29, Loss: 0.5585206747055054\n",
      "Epoch 30, Loss: 0.5715859532356262\n",
      "Epoch 31, Loss: 0.5742624402046204\n",
      "Epoch 32, Loss: 0.5616341233253479\n",
      "Epoch 33, Loss: 0.5560781955718994\n",
      "Epoch 34, Loss: 0.5632168054580688\n",
      "Epoch 35, Loss: 0.5658241510391235\n",
      "Epoch 36, Loss: 0.5574185252189636\n",
      "Epoch 37, Loss: 0.5519223809242249\n",
      "Epoch 38, Loss: 0.5569820404052734\n",
      "Epoch 39, Loss: 0.5596757531166077\n",
      "Epoch 40, Loss: 0.5534218549728394\n",
      "Epoch 41, Loss: 0.5499308705329895\n",
      "Epoch 42, Loss: 0.5534734725952148\n",
      "Epoch 43, Loss: 0.5544405579566956\n",
      "Epoch 44, Loss: 0.5500792264938354\n",
      "Epoch 45, Loss: 0.5482491850852966\n",
      "Epoch 46, Loss: 0.5507677793502808\n",
      "Epoch 47, Loss: 0.5506395697593689\n",
      "Epoch 48, Loss: 0.5473248362541199\n",
      "Epoch 49, Loss: 0.546838104724884\n",
      "Epoch 50, Loss: 0.5485525727272034\n",
      "Epoch 51, Loss: 0.547610342502594\n",
      "Epoch 52, Loss: 0.5453158020973206\n",
      "Epoch 53, Loss: 0.5455569624900818\n",
      "Epoch 54, Loss: 0.5464561581611633\n",
      "Epoch 55, Loss: 0.5451457500457764\n",
      "Epoch 56, Loss: 0.5439285039901733\n",
      "Epoch 57, Loss: 0.5444810390472412\n",
      "Epoch 58, Loss: 0.5444692373275757\n",
      "Epoch 59, Loss: 0.5431888699531555\n",
      "Epoch 60, Loss: 0.542796790599823\n",
      "Epoch 61, Loss: 0.5432432293891907\n",
      "Epoch 62, Loss: 0.5426706671714783\n",
      "Epoch 63, Loss: 0.5417646765708923\n",
      "Epoch 64, Loss: 0.5418411493301392\n",
      "Epoch 65, Loss: 0.5418731570243835\n",
      "Epoch 66, Loss: 0.5411775708198547\n",
      "Epoch 67, Loss: 0.5408283472061157\n",
      "Epoch 68, Loss: 0.5409266352653503\n",
      "Epoch 69, Loss: 0.5405253171920776\n",
      "Epoch 70, Loss: 0.539986789226532\n",
      "Epoch 71, Loss: 0.5399608016014099\n",
      "Epoch 72, Loss: 0.5398511290550232\n",
      "Epoch 73, Loss: 0.539372980594635\n",
      "Epoch 74, Loss: 0.5391308665275574\n",
      "Epoch 75, Loss: 0.5390735864639282\n",
      "Epoch 76, Loss: 0.5387445092201233\n",
      "Epoch 77, Loss: 0.5384348034858704\n",
      "Epoch 78, Loss: 0.5383538603782654\n",
      "Epoch 79, Loss: 0.5381320118904114\n",
      "Epoch 80, Loss: 0.5377945899963379\n",
      "Epoch 81, Loss: 0.5376474857330322\n",
      "Epoch 82, Loss: 0.537509560585022\n",
      "Epoch 83, Loss: 0.5372294187545776\n",
      "Epoch 84, Loss: 0.5370323061943054\n",
      "Epoch 85, Loss: 0.5368970632553101\n",
      "Epoch 86, Loss: 0.536655604839325\n",
      "Epoch 87, Loss: 0.5364401340484619\n",
      "Epoch 88, Loss: 0.5363098978996277\n",
      "Epoch 89, Loss: 0.5361120104789734\n",
      "Epoch 90, Loss: 0.5358908772468567\n",
      "Epoch 91, Loss: 0.5357434153556824\n",
      "Epoch 92, Loss: 0.5355729460716248\n",
      "Epoch 93, Loss: 0.5353686809539795\n",
      "Epoch 94, Loss: 0.535214364528656\n",
      "Epoch 95, Loss: 0.5350555181503296\n",
      "Epoch 96, Loss: 0.5348607301712036\n",
      "Epoch 97, Loss: 0.5347008109092712\n",
      "Epoch 98, Loss: 0.5345523953437805\n",
      "Epoch 99, Loss: 0.534373939037323\n",
      "Epoch 100, Loss: 0.5342128872871399\n",
      "Epoch 101, Loss: 0.5340664386749268\n",
      "Epoch 102, Loss: 0.5338988900184631\n",
      "Epoch 103, Loss: 0.5337413549423218\n",
      "Epoch 104, Loss: 0.5335985422134399\n",
      "Epoch 105, Loss: 0.5334399938583374\n",
      "Epoch 106, Loss: 0.5332854986190796\n",
      "Epoch 107, Loss: 0.5331448912620544\n",
      "Epoch 108, Loss: 0.532994270324707\n",
      "Epoch 109, Loss: 0.5328453779220581\n",
      "Epoch 110, Loss: 0.5327069163322449\n",
      "Epoch 111, Loss: 0.5325618386268616\n",
      "Epoch 112, Loss: 0.5324175357818604\n",
      "Epoch 113, Loss: 0.5322819948196411\n",
      "Epoch 114, Loss: 0.5321419835090637\n",
      "Epoch 115, Loss: 0.5320023894309998\n",
      "Epoch 116, Loss: 0.5318695902824402\n",
      "Epoch 117, Loss: 0.5317341089248657\n",
      "Epoch 118, Loss: 0.5315989851951599\n",
      "Epoch 119, Loss: 0.5314692854881287\n",
      "Epoch 120, Loss: 0.5313377380371094\n",
      "Epoch 121, Loss: 0.5312067866325378\n",
      "Epoch 122, Loss: 0.5310800671577454\n",
      "Epoch 123, Loss: 0.5309520959854126\n",
      "Epoch 124, Loss: 0.5308248996734619\n",
      "Epoch 125, Loss: 0.530701220035553\n",
      "Epoch 126, Loss: 0.5305767059326172\n",
      "Epoch 127, Loss: 0.5304533839225769\n",
      "Epoch 128, Loss: 0.5303323864936829\n",
      "Epoch 129, Loss: 0.5302109718322754\n",
      "Epoch 130, Loss: 0.5300910472869873\n",
      "Epoch 131, Loss: 0.5299729704856873\n",
      "Epoch 132, Loss: 0.5298547148704529\n",
      "Epoch 133, Loss: 0.5297377705574036\n",
      "Epoch 134, Loss: 0.529622495174408\n",
      "Epoch 135, Loss: 0.5295072197914124\n",
      "Epoch 136, Loss: 0.5293933749198914\n",
      "Epoch 137, Loss: 0.5292804837226868\n",
      "Epoch 138, Loss: 0.5291681289672852\n",
      "Epoch 139, Loss: 0.529056966304779\n",
      "Epoch 140, Loss: 0.5289468765258789\n",
      "Epoch 141, Loss: 0.5288370847702026\n",
      "Epoch 142, Loss: 0.5287286043167114\n",
      "Epoch 143, Loss: 0.5286208391189575\n",
      "Epoch 144, Loss: 0.5285136699676514\n",
      "Epoch 145, Loss: 0.5284077525138855\n",
      "Epoch 146, Loss: 0.5283023118972778\n",
      "Epoch 147, Loss: 0.528197705745697\n",
      "Epoch 148, Loss: 0.5280941128730774\n",
      "Epoch 149, Loss: 0.5279910564422607\n",
      "Epoch 150, Loss: 0.5278887748718262\n",
      "Epoch 151, Loss: 0.527787446975708\n",
      "Epoch 152, Loss: 0.5276866555213928\n",
      "Epoch 153, Loss: 0.5275866389274597\n",
      "Epoch 154, Loss: 0.5274874567985535\n",
      "Epoch 155, Loss: 0.5273887515068054\n",
      "Epoch 156, Loss: 0.527290940284729\n",
      "Epoch 157, Loss: 0.5271938443183899\n",
      "Epoch 158, Loss: 0.527097225189209\n",
      "Epoch 159, Loss: 0.5270015597343445\n",
      "Epoch 160, Loss: 0.5269063711166382\n",
      "Epoch 161, Loss: 0.5268118977546692\n",
      "Epoch 162, Loss: 0.5267180800437927\n",
      "Epoch 163, Loss: 0.526624858379364\n",
      "Epoch 164, Loss: 0.5265322923660278\n",
      "Epoch 165, Loss: 0.5264405012130737\n",
      "Epoch 166, Loss: 0.5263491272926331\n",
      "Epoch 167, Loss: 0.5262584686279297\n",
      "Epoch 168, Loss: 0.5261683464050293\n",
      "Epoch 169, Loss: 0.5260789394378662\n",
      "Epoch 170, Loss: 0.5259900093078613\n",
      "Epoch 171, Loss: 0.525901734828949\n",
      "Epoch 172, Loss: 0.5258139967918396\n",
      "Epoch 173, Loss: 0.525726854801178\n",
      "Epoch 174, Loss: 0.5256403088569641\n",
      "Epoch 175, Loss: 0.5255541801452637\n",
      "Epoch 176, Loss: 0.5254687070846558\n",
      "Epoch 177, Loss: 0.5253838896751404\n",
      "Epoch 178, Loss: 0.5252994894981384\n",
      "Epoch 179, Loss: 0.5252156257629395\n",
      "Epoch 180, Loss: 0.5251322984695435\n",
      "Epoch 181, Loss: 0.5250495672225952\n",
      "Epoch 182, Loss: 0.5249671936035156\n",
      "Epoch 183, Loss: 0.5248855352401733\n",
      "Epoch 184, Loss: 0.5248042345046997\n",
      "Epoch 185, Loss: 0.5247235298156738\n",
      "Epoch 186, Loss: 0.5246432423591614\n",
      "Epoch 187, Loss: 0.5245635509490967\n",
      "Epoch 188, Loss: 0.5244842767715454\n",
      "Epoch 189, Loss: 0.5244054198265076\n",
      "Epoch 190, Loss: 0.5243271589279175\n",
      "Epoch 191, Loss: 0.5242493748664856\n",
      "Epoch 192, Loss: 0.5241720676422119\n",
      "Epoch 193, Loss: 0.5240950584411621\n",
      "Epoch 194, Loss: 0.5240185856819153\n",
      "Epoch 195, Loss: 0.5239426493644714\n",
      "Epoch 196, Loss: 0.523867130279541\n",
      "Epoch 197, Loss: 0.523792028427124\n",
      "Epoch 198, Loss: 0.5237174034118652\n",
      "Epoch 199, Loss: 0.5236431360244751\n",
      "Epoch 200, Loss: 0.5235694050788879\n",
      "Epoch 201, Loss: 0.5234960913658142\n",
      "Epoch 202, Loss: 0.5234231948852539\n",
      "Epoch 203, Loss: 0.523350715637207\n",
      "Epoch 204, Loss: 0.5232785940170288\n",
      "Epoch 205, Loss: 0.5232069492340088\n",
      "Epoch 206, Loss: 0.523135781288147\n",
      "Epoch 207, Loss: 0.523064911365509\n",
      "Epoch 208, Loss: 0.5229944586753845\n",
      "Epoch 209, Loss: 0.522924542427063\n",
      "Epoch 210, Loss: 0.5228548645973206\n",
      "Epoch 211, Loss: 0.5227856636047363\n",
      "Epoch 212, Loss: 0.5227168798446655\n",
      "Epoch 213, Loss: 0.5226483941078186\n",
      "Epoch 214, Loss: 0.5225802659988403\n",
      "Epoch 215, Loss: 0.5225125551223755\n",
      "Epoch 216, Loss: 0.5224452018737793\n",
      "Epoch 217, Loss: 0.5223782658576965\n",
      "Epoch 218, Loss: 0.5223117470741272\n",
      "Epoch 219, Loss: 0.5222455859184265\n",
      "Epoch 220, Loss: 0.5221797823905945\n",
      "Epoch 221, Loss: 0.5221143364906311\n",
      "Epoch 222, Loss: 0.5220491290092468\n",
      "Epoch 223, Loss: 0.5219844579696655\n",
      "Epoch 224, Loss: 0.5219200849533081\n",
      "Epoch 225, Loss: 0.5218561291694641\n",
      "Epoch 226, Loss: 0.5217922925949097\n",
      "Epoch 227, Loss: 0.5217289924621582\n",
      "Epoch 228, Loss: 0.5216659903526306\n",
      "Epoch 229, Loss: 0.5216034054756165\n",
      "Epoch 230, Loss: 0.5215409994125366\n",
      "Epoch 231, Loss: 0.5214790105819702\n",
      "Epoch 232, Loss: 0.5214173197746277\n",
      "Epoch 233, Loss: 0.5213560461997986\n",
      "Epoch 234, Loss: 0.5212950706481934\n",
      "Epoch 235, Loss: 0.5212343335151672\n",
      "Epoch 236, Loss: 0.5211741328239441\n",
      "Epoch 237, Loss: 0.5211141109466553\n",
      "Epoch 238, Loss: 0.5210543274879456\n",
      "Epoch 239, Loss: 0.5209950804710388\n",
      "Epoch 240, Loss: 0.5209358930587769\n",
      "Epoch 241, Loss: 0.5208771228790283\n",
      "Epoch 242, Loss: 0.5208187103271484\n",
      "Epoch 243, Loss: 0.5207605957984924\n",
      "Epoch 244, Loss: 0.5207026600837708\n",
      "Epoch 245, Loss: 0.5206451416015625\n",
      "Epoch 246, Loss: 0.5205879211425781\n",
      "Epoch 247, Loss: 0.5205310583114624\n",
      "Epoch 248, Loss: 0.520474374294281\n",
      "Epoch 249, Loss: 0.520418107509613\n",
      "Epoch 250, Loss: 0.5203619599342346\n",
      "Epoch 251, Loss: 0.5203061699867249\n",
      "Epoch 252, Loss: 0.520250678062439\n",
      "Epoch 253, Loss: 0.5201955437660217\n",
      "Epoch 254, Loss: 0.5201405882835388\n",
      "Epoch 255, Loss: 0.5200860500335693\n",
      "Epoch 256, Loss: 0.5200316905975342\n",
      "Epoch 257, Loss: 0.5199775695800781\n",
      "Epoch 258, Loss: 0.519923746585846\n",
      "Epoch 259, Loss: 0.5198702812194824\n",
      "Epoch 260, Loss: 0.5198169946670532\n",
      "Epoch 261, Loss: 0.5197640657424927\n",
      "Epoch 262, Loss: 0.519711434841156\n",
      "Epoch 263, Loss: 0.5196589827537537\n",
      "Epoch 264, Loss: 0.5196068286895752\n",
      "Epoch 265, Loss: 0.519554853439331\n",
      "Epoch 266, Loss: 0.5195032954216003\n",
      "Epoch 267, Loss: 0.5194518566131592\n",
      "Epoch 268, Loss: 0.5194007158279419\n",
      "Epoch 269, Loss: 0.5193499326705933\n",
      "Epoch 270, Loss: 0.5192992687225342\n",
      "Epoch 271, Loss: 0.5192489624023438\n",
      "Epoch 272, Loss: 0.5191988945007324\n",
      "Epoch 273, Loss: 0.5191489458084106\n",
      "Epoch 274, Loss: 0.5190993547439575\n",
      "Epoch 275, Loss: 0.519050121307373\n",
      "Epoch 276, Loss: 0.5190009474754333\n",
      "Epoch 277, Loss: 0.5189520716667175\n",
      "Epoch 278, Loss: 0.5189034342765808\n",
      "Epoch 279, Loss: 0.5188550353050232\n",
      "Epoch 280, Loss: 0.5188068747520447\n",
      "Epoch 281, Loss: 0.51875901222229\n",
      "Epoch 282, Loss: 0.5187113285064697\n",
      "Epoch 283, Loss: 0.5186638832092285\n",
      "Epoch 284, Loss: 0.5186166763305664\n",
      "Epoch 285, Loss: 0.5185697674751282\n",
      "Epoch 286, Loss: 0.5185229182243347\n",
      "Epoch 287, Loss: 0.5184764862060547\n",
      "Epoch 288, Loss: 0.518430233001709\n",
      "Epoch 289, Loss: 0.5183842182159424\n",
      "Epoch 290, Loss: 0.5183383226394653\n",
      "Epoch 291, Loss: 0.5182926654815674\n",
      "Epoch 292, Loss: 0.5182473659515381\n",
      "Epoch 293, Loss: 0.5182021856307983\n",
      "Epoch 294, Loss: 0.5181572437286377\n",
      "Epoch 295, Loss: 0.5181125402450562\n",
      "Epoch 296, Loss: 0.5180680155754089\n",
      "Epoch 297, Loss: 0.5180237293243408\n",
      "Epoch 298, Loss: 0.5179796814918518\n",
      "Epoch 299, Loss: 0.5179356932640076\n",
      "Epoch 300, Loss: 0.517892062664032\n",
      "Accuracy:  0.54\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the sentence transformer model\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "# Assume we have some data\n",
    "# texts = ['This is a good day!', 'I feel very sad', 'I am not happy about this', 'I am thrilled']\n",
    "# labels = [1, 0, 0, 1]  # 1: Positive sentiment, 0: Negative sentiment\n",
    "X_train = df_image_train['text']\n",
    "y_train = df_image_train['label']\n",
    "\n",
    "X_val = df_image_val['text']\n",
    "y_val = df_image_val['label']\n",
    "\n",
    "# X_test = df_image_test['text']\n",
    "# y_test = df_image_test['label']\n",
    "\n",
    "# # Split into training and testing sets\n",
    "# train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2)\n",
    "\n",
    "# Convert the texts to embeddings\n",
    "train_embeddings = model.encode(X_train)\n",
    "test_embeddings = model.encode(X_val)\n",
    "\n",
    "# Convert everything into torch tensors\n",
    "train_embeddings = torch.tensor(train_embeddings)\n",
    "train_labels = torch.tensor(y_train)\n",
    "\n",
    "test_embeddings = torch.tensor(test_embeddings)\n",
    "test_labels = torch.tensor(y_val)\n",
    "\n",
    "# A simple linear classifier on top of SBERT's embeddings\n",
    "classifier = nn.Linear(train_embeddings.size(1), 2)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(classifier.parameters(), lr=1e-2)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(300):  # for simplicity, we train for 10 epochs\n",
    "    classifier.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = classifier(train_embeddings)\n",
    "    loss = criterion(outputs, train_labels)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n",
    "\n",
    "# Test the classifier\n",
    "classifier.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = classifier(test_embeddings)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Accuracy: ', (predicted == test_labels).sum().item() / test_labels.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a696884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     outputs = classifier(train_embeddings)\n",
    "#     _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# print('Accuracy: ', (predicted == train_labels).sum().item() / test_labels.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef62f02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
