{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-06-16T02:43:57.521834Z","iopub.status.busy":"2023-06-16T02:43:57.521474Z","iopub.status.idle":"2023-06-16T02:43:57.656934Z","shell.execute_reply":"2023-06-16T02:43:57.656063Z","shell.execute_reply.started":"2023-06-16T02:43:57.521805Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>img</th>\n","      <th>label</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>42953</td>\n","      <td>img/42953.png</td>\n","      <td>0</td>\n","      <td>its their character not their color that matters</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>23058</td>\n","      <td>img/23058.png</td>\n","      <td>0</td>\n","      <td>don't be afraid to love again everyone is not ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>13894</td>\n","      <td>img/13894.png</td>\n","      <td>0</td>\n","      <td>putting bows on your pet</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>37408</td>\n","      <td>img/37408.png</td>\n","      <td>0</td>\n","      <td>i love everything and everybody! except for sq...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>82403</td>\n","      <td>img/82403.png</td>\n","      <td>0</td>\n","      <td>everybody loves chocolate chip cookies, even h...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      id            img  label  \\\n","0  42953  img/42953.png      0   \n","1  23058  img/23058.png      0   \n","2  13894  img/13894.png      0   \n","3  37408  img/37408.png      0   \n","4  82403  img/82403.png      0   \n","\n","                                                text  \n","0   its their character not their color that matters  \n","1  don't be afraid to love again everyone is not ...  \n","2                           putting bows on your pet  \n","3  i love everything and everybody! except for sq...  \n","4  everybody loves chocolate chip cookies, even h...  "]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","# f = pd.read_json(file_path, lines=True)\n","df_image_train = pd.read_json(\"/kaggle/input/facebook-hateful-meme-dataset/data/train.jsonl\", lines=True)\n","df_image_val = pd.read_json(\"/kaggle/input/facebook-hateful-meme-dataset/data/dev.jsonl\", lines=True)\n","df_image_test = pd.read_json(\"/kaggle/input/facebook-hateful-meme-dataset/data/test.jsonl\", lines=True)\n","df_image_train.head()"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-06-16T02:44:05.727654Z","iopub.status.busy":"2023-06-16T02:44:05.727230Z","iopub.status.idle":"2023-06-16T04:08:52.505419Z","shell.execute_reply":"2023-06-16T04:08:52.503507Z","shell.execute_reply.started":"2023-06-16T02:44:05.727623Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b579be57a1874fb0809a878fdde7cf57","version_major":2,"version_minor":0},"text/plain":["Downloading (…)rocessor_config.json:   0%|          | 0.00/244 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n","caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n","  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n","/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n","caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n","  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","/opt/conda/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"11704528f7cc47f494888a1237106f2f","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/452 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f203ad52edff4733aa3c5272c0522cf1","version_major":2,"version_minor":0},"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/86.8M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of ViTModel were not initialized from the model checkpoint at facebook/dino-vits8 and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 0, Loss: 0.7275950908660889\n","Validation Precision: 0.5081126687435099\n","Validation Recall: 0.5053333333333333\n","Validation Accuracy: 0.682\n","Epoch: 1, Loss: 0.5587728023529053\n","Validation Precision: 0.5065968630596778\n","Validation Recall: 0.5066666666666666\n","Validation Accuracy: 0.628\n","Epoch: 2, Loss: 0.6088539958000183\n","Validation Precision: 0.5328947368421053\n","Validation Recall: 0.532\n","Validation Accuracy: 0.654\n","Epoch: 3, Loss: 0.2793712615966797\n","Validation Precision: 0.5146775632896529\n","Validation Recall: 0.5066666666666667\n","Validation Accuracy: 0.708\n","Epoch: 4, Loss: 0.13934367895126343\n","Validation Precision: 0.5165762507534659\n","Validation Recall: 0.5146666666666667\n","Validation Accuracy: 0.656\n","Epoch: 5, Loss: 0.16861902177333832\n","Validation Precision: 0.49588382507903056\n","Validation Recall: 0.4946666666666667\n","Validation Accuracy: 0.538\n","Epoch: 6, Loss: 0.4310924708843231\n","Validation Precision: 0.5077962577962578\n","Validation Recall: 0.508\n","Validation Accuracy: 0.626\n","Epoch: 7, Loss: 0.0006762255216017365\n","Validation Precision: 0.5081221572449642\n","Validation Recall: 0.5066666666666667\n","Validation Accuracy: 0.66\n","Epoch: 8, Loss: 0.0002564936876296997\n","Validation Precision: 0.4894674755645433\n","Validation Recall: 0.492\n","Validation Accuracy: 0.658\n","Epoch: 9, Loss: 0.03248279541730881\n","Validation Precision: 0.5092373632870234\n","Validation Recall: 0.508\n","Validation Accuracy: 0.654\n","Epoch: 10, Loss: 0.011140908114612103\n","Validation Precision: 0.5014474628870516\n","Validation Recall: 0.5013333333333333\n","Validation Accuracy: 0.64\n","Epoch: 11, Loss: 0.007883112877607346\n","Validation Precision: 0.5046586165772212\n","Validation Recall: 0.5053333333333333\n","Validation Accuracy: 0.598\n","Epoch: 12, Loss: 0.024254925549030304\n","Validation Precision: 0.5285561985988425\n","Validation Recall: 0.524\n","Validation Accuracy: 0.67\n","Epoch: 13, Loss: 0.00023892565513961017\n","Validation Precision: 0.5294672324375295\n","Validation Recall: 0.5253333333333333\n","Validation Accuracy: 0.668\n","Epoch: 14, Loss: 0.2045208364725113\n","Validation Precision: 0.5061213296507414\n","Validation Recall: 0.5066666666666667\n","Validation Accuracy: 0.612\n","Epoch: 15, Loss: 0.0011121374554932117\n","Validation Precision: 0.5119047619047619\n","Validation Recall: 0.5133333333333333\n","Validation Accuracy: 0.61\n","Epoch: 16, Loss: 0.7997291684150696\n","Validation Precision: 0.5163766900744157\n","Validation Recall: 0.5133333333333333\n","Validation Accuracy: 0.666\n","Epoch: 17, Loss: 0.0010543863754719496\n","Validation Precision: 0.4905652239546269\n","Validation Recall: 0.49066666666666664\n","Validation Accuracy: 0.62\n","Epoch: 18, Loss: 0.002138041891157627\n","Validation Precision: 0.49692087890432557\n","Validation Recall: 0.4973333333333333\n","Validation Accuracy: 0.646\n","Epoch: 19, Loss: 0.000967741128988564\n","Validation Precision: 0.5201616728970189\n","Validation Recall: 0.5226666666666667\n","Validation Accuracy: 0.616\n"]}],"source":["import os\n","import pandas as pd\n","import torch\n","import torchvision.transforms as T\n","from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","from transformers import AutoFeatureExtractor, AutoModel\n","from sklearn.metrics import precision_score, recall_score, roc_auc_score, accuracy_score\n","\n","# Read JSON files\n","df_image_train = pd.read_json(\"/kaggle/input/facebook-hateful-meme-dataset/data/train.jsonl\", lines=True)\n","df_image_val = pd.read_json(\"/kaggle/input/facebook-hateful-meme-dataset/data/dev.jsonl\",lines = True)\n","\n","# Get image paths\n","def load_image_paths(df):\n","    image_paths = [os.path.join(\"/kaggle/input/facebook-hateful-meme-dataset/data\", img_path) for img_path in df['img']]\n","    return image_paths\n","\n","# Prepare data\n","X_train_images = load_image_paths(df_image_train)\n","y_train = df_image_train['label']\n","\n","X_val_images = load_image_paths(df_image_val)\n","y_val = df_image_val['label']\n","\n","# Dataset\n","class ImageDataset(Dataset):\n","    def __init__(self, image_paths, labels, transforms=None):\n","        self.image_paths = image_paths\n","        self.labels = labels\n","        self.transforms = transforms\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.image_paths[idx]\n","        image = Image.open(image_path).convert('RGB')\n","        if self.transforms:\n","            image = self.transforms(image)\n","        label = self.labels[idx]\n","        return image, label\n","\n","# Define the transformations\n","transform = T.Compose([\n","    T.Resize((224, 224)),  # Resize to the size expected by DINO\n","    T.ToTensor(),\n","])\n","\n","# Create the datasets\n","train_dataset = ImageDataset(X_train_images, y_train, transforms=transform)\n","\n","# Create the dataloaders\n","train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","\n","# Create the datasets\n","val_dataset = ImageDataset(X_val_images, y_train, transforms=transform)\n","\n","# Create the dataloaders\n","val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n","\n","# Load pretrained models\n","extractor = AutoFeatureExtractor.from_pretrained(\"facebook/dino-vits8\")\n","model = AutoModel.from_pretrained(\"facebook/dino-vits8\")\n","\n","# Add a classification head to the model\n","num_classes = 2\n","model.classifier = nn.Linear(model.config.hidden_size, num_classes)\n","\n","# Set up loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.00001)\n","\n","# Move model to GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = model.to(device)\n","\n","# Training loop\n","num_epochs = 20\n","model.train()\n","\n","for epoch in range(num_epochs):\n","    for images, labels in train_dataloader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        outputs = model(images)\n","        logits = model.classifier(outputs.pooler_output)\n","\n","        loss = criterion(logits, labels)\n","\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","    \n","    print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n","    \n","    model.eval()  # Set the model to evaluation mode\n","    with torch.no_grad():  # Do not calculate gradients (saves memory and computation)\n","        all_labels = []\n","        all_predictions = []\n","\n","        for images, labels in val_dataloader:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(images)\n","            logits = model.classifier(outputs.pooler_output)\n","\n","            _, predicted = torch.max(logits, 1)\n","            all_labels.extend(labels.cpu().numpy())\n","            all_predictions.extend(predicted.cpu().numpy())\n","\n","        precision = precision_score(all_labels, all_predictions, average='macro')\n","        recall = recall_score(all_labels, all_predictions, average='macro')\n","        accuracy = accuracy_score(all_labels, all_predictions)\n","\n","        print(f\"Validation Precision: {precision}\")\n","        print(f\"Validation Recall: {recall}\")\n","        print(f\"Validation Accuracy: {accuracy}\")\n","        \n","        if (epoch + 1) % 4 == 0:\n","            torch.save(model.state_dict(), f\"vit_final_{epoch+1}.pth\")\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-06-16T05:13:50.322593Z","iopub.status.busy":"2023-06-16T05:13:50.322092Z","iopub.status.idle":"2023-06-16T05:14:03.838605Z","shell.execute_reply":"2023-06-16T05:14:03.837610Z","shell.execute_reply.started":"2023-06-16T05:13:50.322557Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of ViTModel were not initialized from the model checkpoint at facebook/dino-vits8 and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Validation Precision: 0.5213476536556672\n","Validation Recall: 0.518\n","Validation Accuracy: 0.518\n","Validation AUROC: 0.512832\n"]}],"source":["#### Code for Validation\n","# Path of the saved model\n","model_path = \"vit_final_20.pth\"  # modify this to your actual path\n","\n","# Initialize the same model architecture\n","model = AutoModel.from_pretrained(\"facebook/dino-vits8\")\n","model.classifier = nn.Linear(model.config.hidden_size, num_classes)\n","model = model.to(device)\n","\n","# Load the saved model parameters\n","model.load_state_dict(torch.load(model_path))\n","\n","df_image_test = pd.read_json(\"/kaggle/input/facebook-hateful-meme-dataset/data/dev.jsonl\", lines=True)\n","X_test_images = load_image_paths(df_image_test)\n","y_test = df_image_test['label']\n","\n","# Create the test dataset and dataloader, assuming that you have already loaded the test data\n","# into X_test_images and y_test\n","test_dataset = ImageDataset(X_test_images, y_test, transforms=transform)\n","test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n","\n","# Testing loop\n","# Testing loop\n","model.eval()  # Set the model to evaluation mode\n","with torch.no_grad():  # Do not calculate gradients (saves memory and computation)\n","    all_labels = []\n","    all_predictions = []\n","    all_probabilities = []\n","\n","    for images, labels in test_dataloader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        outputs = model(images)\n","        logits = model.classifier(outputs.pooler_output)\n","\n","        probabilities = nn.functional.softmax(logits, dim=1)  # probabilities\n","        _, predicted = torch.max(logits, 1)  # predicted class\n","        \n","        all_labels.extend(labels.cpu().numpy())\n","        all_predictions.extend(predicted.cpu().numpy())\n","        all_probabilities.extend(probabilities[:, 1].cpu().numpy())  # probabilities of class 1\n","\n","    precision = precision_score(all_labels, all_predictions, average='macro')\n","    recall = recall_score(all_labels, all_predictions, average='macro')\n","    accuracy = accuracy_score(all_labels, all_predictions)\n","    roc_auc = roc_auc_score(all_labels, all_probabilities)  # calculate roc_auc\n","\n","    print(f\"Validation Precision: {precision}\")\n","    print(f\"Validation Recall: {recall}\")\n","    print(f\"Validation Accuracy: {accuracy}\")\n","    print(f\"Validation AUROC: {roc_auc}\")"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-06-16T05:12:51.835567Z","iopub.status.busy":"2023-06-16T05:12:51.835205Z","iopub.status.idle":"2023-06-16T05:13:05.057504Z","shell.execute_reply":"2023-06-16T05:13:05.056524Z","shell.execute_reply.started":"2023-06-16T05:12:51.835538Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of ViTModel were not initialized from the model checkpoint at facebook/dino-vits8 and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Test Precision: 0.5209017248904153\n","Test Recall: 0.517626538221504\n","Test Accuracy: 0.52\n","Test AUROC: 0.5114016418364244\n"]}],"source":["#### Code for Testing\n","# Path of the saved model\n","model_path = \"vit_final_20.pth\"  # modify this to your actual path\n","\n","# Initialize the same model architecture\n","model = AutoModel.from_pretrained(\"facebook/dino-vits8\")\n","model.classifier = nn.Linear(model.config.hidden_size, num_classes)\n","model = model.to(device)\n","\n","# Load the saved model parameters\n","model.load_state_dict(torch.load(model_path))\n","\n","df_image_test = pd.read_json(\"/kaggle/input/dev-seen-memes/dev_seen.jsonl\", lines=True)\n","X_test_images = load_image_paths(df_image_test)\n","y_test = df_image_test['label']\n","\n","# Create the test dataset and dataloader, assuming that you have already loaded the test data\n","# into X_test_images and y_test\n","test_dataset = ImageDataset(X_test_images, y_test, transforms=transform)\n","test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n","\n","# Testing loop\n","# Testing loop\n","model.eval()  # Set the model to evaluation mode\n","all_predictions_vit = []\n","with torch.no_grad():  # Do not calculate gradients (saves memory and computation)\n","    all_labels = []\n","    all_predictions = []\n","    all_probabilities = []\n","\n","    for images, labels in test_dataloader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        outputs = model(images)\n","        logits = model.classifier(outputs.pooler_output)\n","\n","        probabilities = nn.functional.softmax(logits, dim=1)  # probabilities\n","        _, predicted = torch.max(logits, 1)  # predicted class\n","        \n","        all_labels.extend(labels.cpu().numpy())\n","        all_predictions.extend(predicted.cpu().numpy())\n","        all_probabilities.extend(probabilities[:, 1].cpu().numpy())  # probabilities of class 1\n","\n","    precision = precision_score(all_labels, all_predictions, average='macro')\n","    recall = recall_score(all_labels, all_predictions, average='macro')\n","    accuracy = accuracy_score(all_labels, all_predictions)\n","    roc_auc = roc_auc_score(all_labels, all_probabilities)  # calculate roc_auc\n","\n","    print(f\"Test Precision: {precision}\")\n","    print(f\"Test Recall: {recall}\")\n","    print(f\"Test Accuracy: {accuracy}\")\n","    print(f\"Test AUROC: {roc_auc}\")\n","    all_predictions_vit = all_predictions"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
