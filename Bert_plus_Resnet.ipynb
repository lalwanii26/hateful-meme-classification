{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1186ccd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# !pip install transformers\n",
    "# !pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b68ba118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2851ee37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42953</td>\n",
       "      <td>img/42953.png</td>\n",
       "      <td>0</td>\n",
       "      <td>its their character not their color that matters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23058</td>\n",
       "      <td>img/23058.png</td>\n",
       "      <td>0</td>\n",
       "      <td>don't be afraid to love again everyone is not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13894</td>\n",
       "      <td>img/13894.png</td>\n",
       "      <td>0</td>\n",
       "      <td>putting bows on your pet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37408</td>\n",
       "      <td>img/37408.png</td>\n",
       "      <td>0</td>\n",
       "      <td>i love everything and everybody! except for sq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82403</td>\n",
       "      <td>img/82403.png</td>\n",
       "      <td>0</td>\n",
       "      <td>everybody loves chocolate chip cookies, even h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id            img  label  \\\n",
       "0  42953  img/42953.png      0   \n",
       "1  23058  img/23058.png      0   \n",
       "2  13894  img/13894.png      0   \n",
       "3  37408  img/37408.png      0   \n",
       "4  82403  img/82403.png      0   \n",
       "\n",
       "                                                text  \n",
       "0   its their character not their color that matters  \n",
       "1  don't be afraid to love again everyone is not ...  \n",
       "2                           putting bows on your pet  \n",
       "3  i love everything and everybody! except for sq...  \n",
       "4  everybody loves chocolate chip cookies, even h...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# f = pd.read_json(file_path, lines=True)\n",
    "df_image_train = pd.read_json(\"data/train.jsonl\", lines=True)\n",
    "df_image_val = pd.read_json(\"data/dev.jsonl\", lines=True)\n",
    "df_image_test = pd.read_json(\"data/test.jsonl\", lines=True)\n",
    "df_image_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5906099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c204c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id            img  label  \\\n",
      "0  42953  img/42953.png      0   \n",
      "1  23058  img/23058.png      0   \n",
      "2  13894  img/13894.png      0   \n",
      "3  37408  img/37408.png      0   \n",
      "4  82403  img/82403.png      0   \n",
      "\n",
      "                                                text  \n",
      "0   its their character not their color that matters  \n",
      "1  don't be afraid to love again everyone is not ...  \n",
      "2                           putting bows on your pet  \n",
      "3  i love everything and everybody! except for sq...  \n",
      "4  everybody loves chocolate chip cookies, even h...  \n"
     ]
    }
   ],
   "source": [
    "print(df_image_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65038d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.5685920119285583, Val Loss: 0.7641576902260856, Val Accuracy: 50.4%, Precision: 50.45657096939149%, Recall: 50.4%, AUROC: 0.504\n",
      "Epoch [2/20], Loss: 0.31835445761680603, Val Loss: 0.9863109810957833, Val Accuracy: 52.6%, Precision: 53.81285342218259%, Recall: 52.6%, AUROC: 0.526\n",
      "Epoch [3/20], Loss: 0.5039176940917969, Val Loss: 0.8632743278193096, Val Accuracy: 52.6%, Precision: 53.8643551877482%, Recall: 52.6%, AUROC: 0.526\n",
      "Epoch [4/20], Loss: 0.4317115843296051, Val Loss: 0.769026108677425, Val Accuracy: 55.2%, Precision: 55.3229821802935%, Recall: 55.2%, AUROC: 0.5519999999999999\n",
      "Epoch [5/20], Loss: 0.6014852523803711, Val Loss: 0.8528119131686196, Val Accuracy: 53.0%, Precision: 53.48182949248853%, Recall: 53.0%, AUROC: 0.5299999999999999\n",
      "Epoch [6/20], Loss: 0.4335237443447113, Val Loss: 0.8579541265018402, Val Accuracy: 53.400000000000006%, Precision: 53.756341588446375%, Recall: 53.400000000000006%, AUROC: 0.5339999999999999\n",
      "Epoch [7/20], Loss: 0.5749691724777222, Val Loss: 1.137459194849408, Val Accuracy: 51.6%, Precision: 52.2725206799382%, Recall: 51.6%, AUROC: 0.516\n",
      "Epoch [8/20], Loss: 0.734926164150238, Val Loss: 1.3892464616468974, Val Accuracy: 53.0%, Precision: 53.50631136044881%, Recall: 53.0%, AUROC: 0.53\n",
      "Epoch [9/20], Loss: 0.37307876348495483, Val Loss: 1.5228747422733004, Val Accuracy: 50.4%, Precision: 50.42113065157334%, Recall: 50.4%, AUROC: 0.504\n",
      "Epoch [10/20], Loss: 0.13646526634693146, Val Loss: 1.6099045025923895, Val Accuracy: 51.800000000000004%, Precision: 51.87191134627863%, Recall: 51.800000000000004%, AUROC: 0.518\n",
      "Epoch [11/20], Loss: 0.05861751362681389, Val Loss: 1.8235635161399841, Val Accuracy: 53.0%, Precision: 53.351865424837776%, Recall: 53.0%, AUROC: 0.5299999999999999\n",
      "Epoch [12/20], Loss: 0.005369306541979313, Val Loss: 2.332559345733552, Val Accuracy: 50.0%, Precision: 50.0%, Recall: 50.0%, AUROC: 0.49999999999999994\n",
      "Epoch [13/20], Loss: 0.3322257101535797, Val Loss: 1.9760780062467334, Val Accuracy: 52.800000000000004%, Precision: 52.87356321839082%, Recall: 52.800000000000004%, AUROC: 0.528\n",
      "Epoch [14/20], Loss: 0.40548065304756165, Val Loss: 2.686282186192416, Val Accuracy: 55.00000000000001%, Precision: 56.40224539550511%, Recall: 55.00000000000001%, AUROC: 0.55\n",
      "Epoch [15/20], Loss: 0.032247792929410934, Val Loss: 2.5859217322061934, Val Accuracy: 52.6%, Precision: 53.62634174644618%, Recall: 52.6%, AUROC: 0.526\n",
      "Epoch [16/20], Loss: 0.008058489300310612, Val Loss: 2.0130998837569405, Val Accuracy: 48.199999999999996%, Precision: 48.19163813473501%, Recall: 48.199999999999996%, AUROC: 0.482\n",
      "Epoch [17/20], Loss: 0.0039344849064946175, Val Loss: 2.349093140354232, Val Accuracy: 54.6%, Precision: 55.302569210055516%, Recall: 54.6%, AUROC: 0.5459999999999999\n",
      "Epoch [18/20], Loss: 0.17235861718654633, Val Loss: 2.624630393016906, Val Accuracy: 51.4%, Precision: 52.05307491963678%, Recall: 51.4%, AUROC: 0.514\n",
      "Epoch [19/20], Loss: 0.13181546330451965, Val Loss: 3.0463387739090693, Val Accuracy: 53.0%, Precision: 54.652720911188865%, Recall: 53.0%, AUROC: 0.53\n",
      "Epoch [20/20], Loss: 0.009102925658226013, Val Loss: 2.8208428363478375, Val Accuracy: 53.400000000000006%, Precision: 55.27308369934738%, Recall: 53.400000000000006%, AUROC: 0.5339999999999999\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import torchvision.transforms as T\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "\n",
    "# Transformers model for text\n",
    "text_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Torchvision model for images\n",
    "image_model = models.resnet101(pretrained=True)\n",
    "image_model = nn.Sequential(*list(image_model.children())[:-1])  # Remove the last FC layer\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, texts, labels = zip(*batch)\n",
    "    input_ids = pad_sequence([t['input_ids'] for t in texts])\n",
    "    attention_mask = pad_sequence([t['attention_mask'] for t in texts])\n",
    "\n",
    "    input_ids = torch.as_tensor(input_ids, dtype=torch.long)\n",
    "    attention_mask = torch.as_tensor(attention_mask, dtype=torch.long)\n",
    "    labels = torch.as_tensor(labels, dtype=torch.long)\n",
    "    images = torch.stack(images).transpose(0, 1)\n",
    "    \n",
    "    return images, {'input_ids': input_ids, 'attention_mask': attention_mask}, labels\n",
    "\n",
    "class HatefulMemesDataset(Dataset):\n",
    "    def __init__(self, json_file, img_dir, transforms=None):\n",
    "        self.data = pd.read_json(json_file, lines=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.data.loc[idx, 'img']\n",
    "        img_path = os.path.join(self.img_dir, img_id)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        \n",
    "        text = self.data.loc[idx, 'text']\n",
    "        text = tokenizer(text, return_tensors='pt', padding=\"max_length\", truncation=True, max_length=64)\n",
    "        label = self.data.loc[idx, 'label']\n",
    "        return image, text, label\n",
    "\n",
    "# Define your transforms\n",
    "transforms = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "dataset = HatefulMemesDataset(json_file=\"data/train.jsonl\",\n",
    "                              img_dir=\"data\",\n",
    "                              transforms=transforms)\n",
    "\n",
    "# Compute class weights\n",
    "class_counts = dataset.data['label'].value_counts().to_dict()\n",
    "total_samples = sum(class_counts.values())\n",
    "class_weights = {cls: total_samples/count for cls, count in class_counts.items()}\n",
    "\n",
    "# DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "val_dataset = HatefulMemesDataset(json_file=\"data/dev.jsonl\",\n",
    "                                  img_dir=\"data\",\n",
    "                                  transforms=transforms)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Model for classification\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, text_model, image_model):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.text_model = text_model\n",
    "        self.image_model = image_model\n",
    "        self.classifier = nn.Linear(text_model.config.hidden_size + 2048, 2)\n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        text_features = self.text_model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "        image_features = self.image_model(image).view(image.size(0), -1)\n",
    "        combined = torch.cat((text_features, image_features), dim=1)\n",
    "        logits = self.classifier(combined)\n",
    "        return logits\n",
    "\n",
    "model = CombinedModel(text_model, image_model)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "checkpoint_path = \"combined_model_resnet_bert_epoch_3.pth\"\n",
    "if torch.cuda.is_available():\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=torch.device('cpu')))\n",
    "\n",
    "# Prepare class weights\n",
    "weights = torch.tensor([class_weights[cls] for cls in sorted(class_counts.keys())], dtype=torch.float)\n",
    "weights = weights.to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, texts, labels in dataloader:\n",
    "        images = images.permute(1, 0, 2, 3)\n",
    "        images = images.to(device)\n",
    "        input_ids = texts['input_ids'].squeeze().to(device)\n",
    "        attention_mask = texts['attention_mask'].squeeze().to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        logits = model(images, input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Validation after each epoch\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_labels = []\n",
    "        all_predictions = []\n",
    "        for images, texts, labels in val_dataloader:\n",
    "            images = images.permute(1, 0, 2, 3)\n",
    "            images = images.to(device)\n",
    "            input_ids = texts['input_ids'].squeeze().to(device)\n",
    "            attention_mask = texts['attention_mask'].squeeze().to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(images, input_ids, attention_mask)\n",
    "            val_loss = criterion(logits, labels)\n",
    "            val_losses.append(val_loss.item())\n",
    "            \n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "        avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "        val_acc = correct / total\n",
    "        precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "        recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "        roc_auc = roc_auc_score(all_labels, all_predictions)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}, Val Loss: {avg_val_loss}, Val Accuracy: {val_acc * 100}%, Precision: {precision * 100}%, Recall: {recall * 100}%, AUROC: {roc_auc}')\n",
    "\n",
    "    # Save the model after each epoch\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        torch.save(model.state_dict(), f\"combined_model_resnet_bert_class_weights_epoch_{epoch+1}.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
