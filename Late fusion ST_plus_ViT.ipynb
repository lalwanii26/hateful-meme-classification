{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\n# f = pd.read_json(file_path, lines=True)\ndf_image_train = pd.read_json(\"/kaggle/input/facebook-hateful-meme-dataset/data/train.jsonl\", lines=True)\ndf_image_val = pd.read_json(\"/kaggle/input/facebook-hateful-meme-dataset/data/dev.jsonl\", lines=True)\ndf_image_test = pd.read_json(\"/kaggle/input/facebook-hateful-meme-dataset/data/test.jsonl\", lines=True)\ndf_image_train.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-16T02:43:57.521474Z","iopub.execute_input":"2023-06-16T02:43:57.521834Z","iopub.status.idle":"2023-06-16T02:43:57.656934Z","shell.execute_reply.started":"2023-06-16T02:43:57.521805Z","shell.execute_reply":"2023-06-16T02:43:57.656063Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"      id            img  label  \\\n0  42953  img/42953.png      0   \n1  23058  img/23058.png      0   \n2  13894  img/13894.png      0   \n3  37408  img/37408.png      0   \n4  82403  img/82403.png      0   \n\n                                                text  \n0   its their character not their color that matters  \n1  don't be afraid to love again everyone is not ...  \n2                           putting bows on your pet  \n3  i love everything and everybody! except for sq...  \n4  everybody loves chocolate chip cookies, even h...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>img</th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>42953</td>\n      <td>img/42953.png</td>\n      <td>0</td>\n      <td>its their character not their color that matters</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>23058</td>\n      <td>img/23058.png</td>\n      <td>0</td>\n      <td>don't be afraid to love again everyone is not ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13894</td>\n      <td>img/13894.png</td>\n      <td>0</td>\n      <td>putting bows on your pet</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>37408</td>\n      <td>img/37408.png</td>\n      <td>0</td>\n      <td>i love everything and everybody! except for sq...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>82403</td>\n      <td>img/82403.png</td>\n      <td>0</td>\n      <td>everybody loves chocolate chip cookies, even h...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport torch\nimport torchvision.transforms as T\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom transformers import AutoFeatureExtractor, AutoModel\nfrom sklearn.metrics import precision_score, recall_score, roc_auc_score, accuracy_score\n\n# Read JSON files\ndf_image_train = pd.read_json(\"/kaggle/input/facebook-hateful-meme-dataset/data/train.jsonl\", lines=True)\ndf_image_val = pd.read_json(\"/kaggle/input/facebook-hateful-meme-dataset/data/dev.jsonl\",lines = True)\n\n# Get image paths\ndef load_image_paths(df):\n    image_paths = [os.path.join(\"/kaggle/input/facebook-hateful-meme-dataset/data\", img_path) for img_path in df['img']]\n    return image_paths\n\n# Prepare data\nX_train_images = load_image_paths(df_image_train)\ny_train = df_image_train['label']\n\nX_val_images = load_image_paths(df_image_val)\ny_val = df_image_val['label']\n\n# Dataset\nclass ImageDataset(Dataset):\n    def __init__(self, image_paths, labels, transforms=None):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        image = Image.open(image_path).convert('RGB')\n        if self.transforms:\n            image = self.transforms(image)\n        label = self.labels[idx]\n        return image, label\n\n# Define the transformations\ntransform = T.Compose([\n    T.Resize((224, 224)),  # Resize to the size expected by DINO\n    T.ToTensor(),\n])\n\n# Create the datasets\ntrain_dataset = ImageDataset(X_train_images, y_train, transforms=transform)\n\n# Create the dataloaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\n# Create the datasets\nval_dataset = ImageDataset(X_val_images, y_train, transforms=transform)\n\n# Create the dataloaders\nval_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n\n# Load pretrained models\nextractor = AutoFeatureExtractor.from_pretrained(\"facebook/dino-vits8\")\nmodel = AutoModel.from_pretrained(\"facebook/dino-vits8\")\n\n# Add a classification head to the model\nnum_classes = 2\nmodel.classifier = nn.Linear(model.config.hidden_size, num_classes)\n\n# Set up loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.00001)\n\n# Move model to GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n\n# Training loop\nnum_epochs = 20\nmodel.train()\n\nfor epoch in range(num_epochs):\n    for images, labels in train_dataloader:\n        images = images.to(device)\n        labels = labels.to(device)\n\n        outputs = model(images)\n        logits = model.classifier(outputs.pooler_output)\n\n        loss = criterion(logits, labels)\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    \n    print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n    \n    model.eval()  # Set the model to evaluation mode\n    with torch.no_grad():  # Do not calculate gradients (saves memory and computation)\n        all_labels = []\n        all_predictions = []\n\n        for images, labels in val_dataloader:\n            images = images.to(device)\n            labels = labels.to(device)\n\n            outputs = model(images)\n            logits = model.classifier(outputs.pooler_output)\n\n            _, predicted = torch.max(logits, 1)\n            all_labels.extend(labels.cpu().numpy())\n            all_predictions.extend(predicted.cpu().numpy())\n\n        precision = precision_score(all_labels, all_predictions, average='macro')\n        recall = recall_score(all_labels, all_predictions, average='macro')\n        accuracy = accuracy_score(all_labels, all_predictions)\n\n        print(f\"Validation Precision: {precision}\")\n        print(f\"Validation Recall: {recall}\")\n        print(f\"Validation Accuracy: {accuracy}\")\n        \n        if (epoch + 1) % 4 == 0:\n            torch.save(model.state_dict(), f\"vit_final_{epoch+1}.pth\")\n","metadata":{"execution":{"iopub.status.busy":"2023-06-16T02:44:05.727230Z","iopub.execute_input":"2023-06-16T02:44:05.727654Z","iopub.status.idle":"2023-06-16T04:08:52.505419Z","shell.execute_reply.started":"2023-06-16T02:44:05.727623Z","shell.execute_reply":"2023-06-16T04:08:52.503507Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)rocessor_config.json:   0%|          | 0.00/244 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b579be57a1874fb0809a878fdde7cf57"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n/opt/conda/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/452 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11704528f7cc47f494888a1237106f2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/86.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f203ad52edff4733aa3c5272c0522cf1"}},"metadata":{}},{"name":"stderr","text":"Some weights of ViTModel were not initialized from the model checkpoint at facebook/dino-vits8 and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Loss: 0.7275950908660889\nValidation Precision: 0.5081126687435099\nValidation Recall: 0.5053333333333333\nValidation Accuracy: 0.682\nEpoch: 1, Loss: 0.5587728023529053\nValidation Precision: 0.5065968630596778\nValidation Recall: 0.5066666666666666\nValidation Accuracy: 0.628\nEpoch: 2, Loss: 0.6088539958000183\nValidation Precision: 0.5328947368421053\nValidation Recall: 0.532\nValidation Accuracy: 0.654\nEpoch: 3, Loss: 0.2793712615966797\nValidation Precision: 0.5146775632896529\nValidation Recall: 0.5066666666666667\nValidation Accuracy: 0.708\nEpoch: 4, Loss: 0.13934367895126343\nValidation Precision: 0.5165762507534659\nValidation Recall: 0.5146666666666667\nValidation Accuracy: 0.656\nEpoch: 5, Loss: 0.16861902177333832\nValidation Precision: 0.49588382507903056\nValidation Recall: 0.4946666666666667\nValidation Accuracy: 0.538\nEpoch: 6, Loss: 0.4310924708843231\nValidation Precision: 0.5077962577962578\nValidation Recall: 0.508\nValidation Accuracy: 0.626\nEpoch: 7, Loss: 0.0006762255216017365\nValidation Precision: 0.5081221572449642\nValidation Recall: 0.5066666666666667\nValidation Accuracy: 0.66\nEpoch: 8, Loss: 0.0002564936876296997\nValidation Precision: 0.4894674755645433\nValidation Recall: 0.492\nValidation Accuracy: 0.658\nEpoch: 9, Loss: 0.03248279541730881\nValidation Precision: 0.5092373632870234\nValidation Recall: 0.508\nValidation Accuracy: 0.654\nEpoch: 10, Loss: 0.011140908114612103\nValidation Precision: 0.5014474628870516\nValidation Recall: 0.5013333333333333\nValidation Accuracy: 0.64\nEpoch: 11, Loss: 0.007883112877607346\nValidation Precision: 0.5046586165772212\nValidation Recall: 0.5053333333333333\nValidation Accuracy: 0.598\nEpoch: 12, Loss: 0.024254925549030304\nValidation Precision: 0.5285561985988425\nValidation Recall: 0.524\nValidation Accuracy: 0.67\nEpoch: 13, Loss: 0.00023892565513961017\nValidation Precision: 0.5294672324375295\nValidation Recall: 0.5253333333333333\nValidation Accuracy: 0.668\nEpoch: 14, Loss: 0.2045208364725113\nValidation Precision: 0.5061213296507414\nValidation Recall: 0.5066666666666667\nValidation Accuracy: 0.612\nEpoch: 15, Loss: 0.0011121374554932117\nValidation Precision: 0.5119047619047619\nValidation Recall: 0.5133333333333333\nValidation Accuracy: 0.61\nEpoch: 16, Loss: 0.7997291684150696\nValidation Precision: 0.5163766900744157\nValidation Recall: 0.5133333333333333\nValidation Accuracy: 0.666\nEpoch: 17, Loss: 0.0010543863754719496\nValidation Precision: 0.4905652239546269\nValidation Recall: 0.49066666666666664\nValidation Accuracy: 0.62\nEpoch: 18, Loss: 0.002138041891157627\nValidation Precision: 0.49692087890432557\nValidation Recall: 0.4973333333333333\nValidation Accuracy: 0.646\nEpoch: 19, Loss: 0.000967741128988564\nValidation Precision: 0.5201616728970189\nValidation Recall: 0.5226666666666667\nValidation Accuracy: 0.616\n","output_type":"stream"}]},{"cell_type":"code","source":"#### Code for Validation\n# Path of the saved model\nmodel_path = \"vit_final_20.pth\"  # modify this to your actual path\n\n# Initialize the same model architecture\nmodel = AutoModel.from_pretrained(\"facebook/dino-vits8\")\nmodel.classifier = nn.Linear(model.config.hidden_size, num_classes)\nmodel = model.to(device)\n\n# Load the saved model parameters\nmodel.load_state_dict(torch.load(model_path))\n\ndf_image_test = pd.read_json(\"/kaggle/input/facebook-hateful-meme-dataset/data/dev.jsonl\", lines=True)\nX_test_images = load_image_paths(df_image_test)\ny_test = df_image_test['label']\n\n# Create the test dataset and dataloader, assuming that you have already loaded the test data\n# into X_test_images and y_test\ntest_dataset = ImageDataset(X_test_images, y_test, transforms=transform)\ntest_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\n# Testing loop\n# Testing loop\nmodel.eval()  # Set the model to evaluation mode\nwith torch.no_grad():  # Do not calculate gradients (saves memory and computation)\n    all_labels = []\n    all_predictions = []\n    all_probabilities = []\n\n    for images, labels in test_dataloader:\n        images = images.to(device)\n        labels = labels.to(device)\n\n        outputs = model(images)\n        logits = model.classifier(outputs.pooler_output)\n\n        probabilities = nn.functional.softmax(logits, dim=1)  # probabilities\n        _, predicted = torch.max(logits, 1)  # predicted class\n        \n        all_labels.extend(labels.cpu().numpy())\n        all_predictions.extend(predicted.cpu().numpy())\n        all_probabilities.extend(probabilities[:, 1].cpu().numpy())  # probabilities of class 1\n\n    precision = precision_score(all_labels, all_predictions, average='macro')\n    recall = recall_score(all_labels, all_predictions, average='macro')\n    accuracy = accuracy_score(all_labels, all_predictions)\n    roc_auc = roc_auc_score(all_labels, all_probabilities)  # calculate roc_auc\n\n    print(f\"Validation Precision: {precision}\")\n    print(f\"Validation Recall: {recall}\")\n    print(f\"Validation Accuracy: {accuracy}\")\n    print(f\"Validation AUROC: {roc_auc}\")","metadata":{"execution":{"iopub.status.busy":"2023-06-16T05:13:50.322092Z","iopub.execute_input":"2023-06-16T05:13:50.322593Z","iopub.status.idle":"2023-06-16T05:14:03.838605Z","shell.execute_reply.started":"2023-06-16T05:13:50.322557Z","shell.execute_reply":"2023-06-16T05:14:03.837610Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Some weights of ViTModel were not initialized from the model checkpoint at facebook/dino-vits8 and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Validation Precision: 0.5213476536556672\nValidation Recall: 0.518\nValidation Accuracy: 0.518\nValidation AUROC: 0.512832\n","output_type":"stream"}]},{"cell_type":"code","source":"#### Code for Testing\n# Path of the saved model\nmodel_path = \"vit_final_20.pth\"  # modify this to your actual path\n\n# Initialize the same model architecture\nmodel = AutoModel.from_pretrained(\"facebook/dino-vits8\")\nmodel.classifier = nn.Linear(model.config.hidden_size, num_classes)\nmodel = model.to(device)\n\n# Load the saved model parameters\nmodel.load_state_dict(torch.load(model_path))\n\ndf_image_test = pd.read_json(\"/kaggle/input/dev-seen-memes/dev_seen.jsonl\", lines=True)\nX_test_images = load_image_paths(df_image_test)\ny_test = df_image_test['label']\n\n# Create the test dataset and dataloader, assuming that you have already loaded the test data\n# into X_test_images and y_test\ntest_dataset = ImageDataset(X_test_images, y_test, transforms=transform)\ntest_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\n# Testing loop\n# Testing loop\nmodel.eval()  # Set the model to evaluation mode\nall_predictions_vit = []\nwith torch.no_grad():  # Do not calculate gradients (saves memory and computation)\n    all_labels = []\n    all_predictions = []\n    all_probabilities = []\n\n    for images, labels in test_dataloader:\n        images = images.to(device)\n        labels = labels.to(device)\n\n        outputs = model(images)\n        logits = model.classifier(outputs.pooler_output)\n\n        probabilities = nn.functional.softmax(logits, dim=1)  # probabilities\n        _, predicted = torch.max(logits, 1)  # predicted class\n        \n        all_labels.extend(labels.cpu().numpy())\n        all_predictions.extend(predicted.cpu().numpy())\n        all_probabilities.extend(probabilities[:, 1].cpu().numpy())  # probabilities of class 1\n\n    precision = precision_score(all_labels, all_predictions, average='macro')\n    recall = recall_score(all_labels, all_predictions, average='macro')\n    accuracy = accuracy_score(all_labels, all_predictions)\n    roc_auc = roc_auc_score(all_labels, all_probabilities)  # calculate roc_auc\n\n    print(f\"Test Precision: {precision}\")\n    print(f\"Test Recall: {recall}\")\n    print(f\"Test Accuracy: {accuracy}\")\n    print(f\"Test AUROC: {roc_auc}\")\n    all_predictions_vit = all_predictions","metadata":{"execution":{"iopub.status.busy":"2023-06-16T05:18:34.806828Z","iopub.execute_input":"2023-06-16T05:18:34.807220Z","iopub.status.idle":"2023-06-16T05:18:48.228031Z","shell.execute_reply.started":"2023-06-16T05:18:34.807167Z","shell.execute_reply":"2023-06-16T05:18:48.227086Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"Some weights of ViTModel were not initialized from the model checkpoint at facebook/dino-vits8 and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Test Precision: 0.5209017248904153\nTest Recall: 0.517626538221504\nTest Accuracy: 0.52\nTest AUROC: 0.5114016418364244\n","output_type":"stream"}]},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n#Our sentences we like to encode\nsentences = ['This framework generates embeddings for each input sentence',\n    'Sentences are passed as a list of string.',\n    'The quick brown fox jumps over the lazy dog.']\n\n#Sentences are encoded by calling model.encode()\nembeddings = model.encode(sentences)\n\n#Print the embeddings\nfor sentence, embedding in zip(sentences, embeddings):\n    print(\"Sentence:\", sentence)\n    print(\"Embedding:\", embedding)\n    print(\"\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nfrom torch.utils.data import DataLoader\nfrom transformers import AdamW\nimport torch\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, roc_auc_score\n\n# Load the sentence transformer model\nmodel = SentenceTransformer('bert-base-nli-mean-tokens')\n\n# Assume we have some data\n# texts = ['This is a good day!', 'I feel very sad', 'I am not happy about this', 'I am thrilled']\n# labels = [1, 0, 0, 1]  # 1: Positive sentiment, 0: Negative sentiment\nX_train = df_image_train['text']\ny_train = df_image_train['label']\n\nX_val = df_image_val['text']\ny_val = df_image_val['label']\n\n# X_test = df_image_test['text']\n# y_test = df_image_test['label']\n\n# # Split into training and testing sets\n# train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2)\n\n# Convert the texts to embeddings\ntrain_embeddings = model.encode(X_train)\nval_embeddings = model.encode(X_val)\n\n# Convert everything into torch tensors\ntrain_embeddings = torch.tensor(train_embeddings)\ntrain_labels = torch.tensor(y_train)\n\nval_embeddings = torch.tensor(val_embeddings)\nval_labels = torch.tensor(y_val)\n\n# A simple linear classifier on top of SBERT's embeddings\nclassifier = nn.Linear(train_embeddings.size(1), 2)\n\n# Loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Optimizer\noptimizer = AdamW(classifier.parameters(), lr=1e-2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nfrom torch.utils.data import DataLoader\nfrom transformers import AdamW\nimport torch\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, roc_auc_score\n\n# Load the sentence transformer model\nmodel = SentenceTransformer('bert-base-nli-mean-tokens')\n\n# Assume we have some data\n# texts = ['This is a good day!', 'I feel very sad', 'I am not happy about this', 'I am thrilled']\n# labels = [1, 0, 0, 1]  # 1: Positive sentiment, 0: Negative sentiment\nX_train = df_image_train['text']\ny_train = df_image_train['label']\n\nX_val = df_image_val['text']\ny_val = df_image_val['label']\n\n# X_test = df_image_test['text']\n# y_test = df_image_test['label']\n\n# # Split into training and testing sets\n# train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2)\n\n# Convert the texts to embeddings\ntrain_embeddings = model.encode(X_train)\nval_embeddings = model.encode(X_val)\n\n# Convert everything into torch tensors\ntrain_embeddings = torch.tensor(train_embeddings)\ntrain_labels = torch.tensor(y_train)\n\nval_embeddings = torch.tensor(val_embeddings)\nval_labels = torch.tensor(y_val)\n\n# A simple linear classifier on top of SBERT's embeddings\nclassifier = nn.Linear(train_embeddings.size(1), 2)\n\n# Loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Optimizer\noptimizer = AdamW(classifier.parameters(), lr=1e-2)\n\n# Training loop\nfor epoch in range(300):  # for simplicity, we train for 10 epochs\n    classifier.train()\n    optimizer.zero_grad()\n\n    # Forward pass\n    outputs = classifier(train_embeddings)\n    loss = criterion(outputs, train_labels)\n\n    # Backward pass\n    loss.backward()\n    optimizer.step()\n\n    print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n\n# Test the classifier\nclassifier.eval()\n\nwith torch.no_grad():\n    outputs = classifier(val_embeddings)\n    _, predicted = torch.max(outputs, 1)\n\n# Compute precision, recall, and AUROC\nprecision = precision_score(val_labels, predicted, average='weighted')\nrecall = recall_score(val_labels, predicted, average='weighted')\nroc_auc = roc_auc_score(val_labels, predicted)\n\nprint('Accuracy: ', (predicted == val_labels).sum().item() / val_labels.size(0))\nprint('Precision: ', precision)\nprint('Recall: ', recall)\nprint('AUROC: ', roc_auc)","metadata":{"execution":{"iopub.status.busy":"2023-06-16T05:39:48.717206Z","iopub.execute_input":"2023-06-16T05:39:48.718209Z","iopub.status.idle":"2023-06-16T05:40:00.211301Z","shell.execute_reply.started":"2023-06-16T05:39:48.718151Z","shell.execute_reply":"2023-06-16T05:40:00.210240Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/266 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8196bd5115d41229c6214671f75cbd2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"545c6dd3aa224a739fe382195c8dd685"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 0.7265491485595703\nEpoch 2, Loss: 1.2745249271392822\nEpoch 3, Loss: 0.778676450252533\nEpoch 4, Loss: 0.7722920775413513\nEpoch 5, Loss: 0.9482970833778381\nEpoch 6, Loss: 0.762198805809021\nEpoch 7, Loss: 0.6077641248703003\nEpoch 8, Loss: 0.6944975256919861\nEpoch 9, Loss: 0.7661585807800293\nEpoch 10, Loss: 0.7080302238464355\nEpoch 11, Loss: 0.6206520199775696\nEpoch 12, Loss: 0.6331160068511963\nEpoch 13, Loss: 0.6878238320350647\nEpoch 14, Loss: 0.6559700965881348\nEpoch 15, Loss: 0.5879985094070435\nEpoch 16, Loss: 0.5846181511878967\nEpoch 17, Loss: 0.6267794370651245\nEpoch 18, Loss: 0.6408953070640564\nEpoch 19, Loss: 0.6120404601097107\nEpoch 20, Loss: 0.583114504814148\nEpoch 21, Loss: 0.5888933539390564\nEpoch 22, Loss: 0.603390097618103\nEpoch 23, Loss: 0.5888254046440125\nEpoch 24, Loss: 0.566001832485199\nEpoch 25, Loss: 0.5715963840484619\nEpoch 26, Loss: 0.5888733267784119\nEpoch 27, Loss: 0.5851156711578369\nEpoch 28, Loss: 0.5662925839424133\nEpoch 29, Loss: 0.5609276294708252\nEpoch 30, Loss: 0.5702297687530518\nEpoch 31, Loss: 0.5703938007354736\nEpoch 32, Loss: 0.560502290725708\nEpoch 33, Loss: 0.55867999792099\nEpoch 34, Loss: 0.5651142597198486\nEpoch 35, Loss: 0.5648412108421326\nEpoch 36, Loss: 0.5562059283256531\nEpoch 37, Loss: 0.5525877475738525\nEpoch 38, Loss: 0.5574209690093994\nEpoch 39, Loss: 0.5583819150924683\nEpoch 40, Loss: 0.5530397295951843\nEpoch 41, Loss: 0.5517973899841309\nEpoch 42, Loss: 0.5545870065689087\nEpoch 43, Loss: 0.5531337261199951\nEpoch 44, Loss: 0.5490307211875916\nEpoch 45, Loss: 0.5492545962333679\nEpoch 46, Loss: 0.5514239072799683\nEpoch 47, Loss: 0.5497615933418274\nEpoch 48, Loss: 0.5471960306167603\nEpoch 49, Loss: 0.5478326082229614\nEpoch 50, Loss: 0.5485361814498901\nEpoch 51, Loss: 0.5467159152030945\nEpoch 52, Loss: 0.5455512404441833\nEpoch 53, Loss: 0.5464250445365906\nEpoch 54, Loss: 0.5461276769638062\nEpoch 55, Loss: 0.5445013642311096\nEpoch 56, Loss: 0.5443408489227295\nEpoch 57, Loss: 0.544852614402771\nEpoch 58, Loss: 0.5440074801445007\nEpoch 59, Loss: 0.5430653691291809\nEpoch 60, Loss: 0.5432635545730591\nEpoch 61, Loss: 0.5431226491928101\nEpoch 62, Loss: 0.5422123074531555\nEpoch 63, Loss: 0.541957676410675\nEpoch 64, Loss: 0.5420895218849182\nEpoch 65, Loss: 0.5415223240852356\nEpoch 66, Loss: 0.540958046913147\nEpoch 67, Loss: 0.5410140752792358\nEpoch 68, Loss: 0.5408177971839905\nEpoch 69, Loss: 0.5402583479881287\nEpoch 70, Loss: 0.5400785207748413\nEpoch 71, Loss: 0.5399792790412903\nEpoch 72, Loss: 0.5395314693450928\nEpoch 73, Loss: 0.5392382144927979\nEpoch 74, Loss: 0.5391924381256104\nEpoch 75, Loss: 0.5388970971107483\nEpoch 76, Loss: 0.5385342836380005\nEpoch 77, Loss: 0.5384082198143005\nEpoch 78, Loss: 0.5382087826728821\nEpoch 79, Loss: 0.5378890037536621\nEpoch 80, Loss: 0.5377255082130432\nEpoch 81, Loss: 0.5375640988349915\nEpoch 82, Loss: 0.5372651219367981\nEpoch 83, Loss: 0.5370578765869141\nEpoch 84, Loss: 0.5369234681129456\nEpoch 85, Loss: 0.5366886854171753\nEpoch 86, Loss: 0.5364704132080078\nEpoch 87, Loss: 0.5363168120384216\nEpoch 88, Loss: 0.5360987782478333\nEpoch 89, Loss: 0.5358826518058777\nEpoch 90, Loss: 0.5357370972633362\nEpoch 91, Loss: 0.5355533361434937\nEpoch 92, Loss: 0.535342812538147\nEpoch 93, Loss: 0.5351808071136475\nEpoch 94, Loss: 0.5350067019462585\nEpoch 95, Loss: 0.5348135828971863\nEpoch 96, Loss: 0.5346574187278748\nEpoch 97, Loss: 0.5344932675361633\nEpoch 98, Loss: 0.5343051552772522\nEpoch 99, Loss: 0.5341445207595825\nEpoch 100, Loss: 0.5339887142181396\nEpoch 101, Loss: 0.533816397190094\nEpoch 102, Loss: 0.5336604714393616\nEpoch 103, Loss: 0.5335059762001038\nEpoch 104, Loss: 0.5333380699157715\nEpoch 105, Loss: 0.5331853628158569\nEpoch 106, Loss: 0.5330379605293274\nEpoch 107, Loss: 0.5328799486160278\nEpoch 108, Loss: 0.5327303409576416\nEpoch 109, Loss: 0.5325843095779419\nEpoch 110, Loss: 0.5324321389198303\nEpoch 111, Loss: 0.5322883725166321\nEpoch 112, Loss: 0.5321471095085144\nEpoch 113, Loss: 0.5320001840591431\nEpoch 114, Loss: 0.5318591594696045\nEpoch 115, Loss: 0.5317208170890808\nEpoch 116, Loss: 0.5315797924995422\nEpoch 117, Loss: 0.5314437747001648\nEpoch 118, Loss: 0.5313085317611694\nEpoch 119, Loss: 0.531171441078186\nEpoch 120, Loss: 0.5310389399528503\nEpoch 121, Loss: 0.5309073328971863\nEpoch 122, Loss: 0.5307751297950745\nEpoch 123, Loss: 0.5306461453437805\nEpoch 124, Loss: 0.5305175185203552\nEpoch 125, Loss: 0.5303889513015747\nEpoch 126, Loss: 0.5302635431289673\nEpoch 127, Loss: 0.5301383137702942\nEpoch 128, Loss: 0.5300135612487793\nEpoch 129, Loss: 0.5298911333084106\nEpoch 130, Loss: 0.5297689437866211\nEpoch 131, Loss: 0.5296477675437927\nEpoch 132, Loss: 0.5295285582542419\nEpoch 133, Loss: 0.5294093489646912\nEpoch 134, Loss: 0.5292913317680359\nEpoch 135, Loss: 0.5291748642921448\nEpoch 136, Loss: 0.5290587544441223\nEpoch 137, Loss: 0.5289439558982849\nEpoch 138, Loss: 0.5288301706314087\nEpoch 139, Loss: 0.5287169814109802\nEpoch 140, Loss: 0.5286049246788025\nEpoch 141, Loss: 0.5284938216209412\nEpoch 142, Loss: 0.5283833742141724\nEpoch 143, Loss: 0.5282741189002991\nEpoch 144, Loss: 0.5281655788421631\nEpoch 145, Loss: 0.528057873249054\nEpoch 146, Loss: 0.527951180934906\nEpoch 147, Loss: 0.5278450846672058\nEpoch 148, Loss: 0.5277400016784668\nEpoch 149, Loss: 0.5276356339454651\nEpoch 150, Loss: 0.5275320410728455\nEpoch 151, Loss: 0.5274292826652527\nEpoch 152, Loss: 0.527327299118042\nEpoch 153, Loss: 0.5272260904312134\nEpoch 154, Loss: 0.5271256566047668\nEpoch 155, Loss: 0.5270259380340576\nEpoch 156, Loss: 0.5269269347190857\nEpoch 157, Loss: 0.5268288254737854\nEpoch 158, Loss: 0.5267313122749329\nEpoch 159, Loss: 0.5266344547271729\nEpoch 160, Loss: 0.5265384912490845\nEpoch 161, Loss: 0.5264430046081543\nEpoch 162, Loss: 0.5263484120368958\nEpoch 163, Loss: 0.5262542963027954\nEpoch 164, Loss: 0.5261609554290771\nEpoch 165, Loss: 0.5260683298110962\nEpoch 166, Loss: 0.5259762406349182\nEpoch 167, Loss: 0.5258848071098328\nEpoch 168, Loss: 0.5257940888404846\nEpoch 169, Loss: 0.525704026222229\nEpoch 170, Loss: 0.5256145596504211\nEpoch 171, Loss: 0.525525689125061\nEpoch 172, Loss: 0.5254374146461487\nEpoch 173, Loss: 0.5253497362136841\nEpoch 174, Loss: 0.525262713432312\nEpoch 175, Loss: 0.5251761674880981\nEpoch 176, Loss: 0.5250903367996216\nEpoch 177, Loss: 0.525005042552948\nEpoch 178, Loss: 0.5249202847480774\nEpoch 179, Loss: 0.5248361229896545\nEpoch 180, Loss: 0.5247525572776794\nEpoch 181, Loss: 0.5246694684028625\nEpoch 182, Loss: 0.5245869159698486\nEpoch 183, Loss: 0.5245050191879272\nEpoch 184, Loss: 0.5244235396385193\nEpoch 185, Loss: 0.5243425965309143\nEpoch 186, Loss: 0.5242622494697571\nEpoch 187, Loss: 0.5241823196411133\nEpoch 188, Loss: 0.524103045463562\nEpoch 189, Loss: 0.5240240693092346\nEpoch 190, Loss: 0.5239458680152893\nEpoch 191, Loss: 0.5238679647445679\nEpoch 192, Loss: 0.5237905979156494\nEpoch 193, Loss: 0.5237137675285339\nEpoch 194, Loss: 0.5236373543739319\nEpoch 195, Loss: 0.523561418056488\nEpoch 196, Loss: 0.5234859585762024\nEpoch 197, Loss: 0.5234110951423645\nEpoch 198, Loss: 0.5233365893363953\nEpoch 199, Loss: 0.5232625603675842\nEpoch 200, Loss: 0.5231889486312866\nEpoch 201, Loss: 0.5231158137321472\nEpoch 202, Loss: 0.523043155670166\nEpoch 203, Loss: 0.5229709148406982\nEpoch 204, Loss: 0.5228991508483887\nEpoch 205, Loss: 0.5228277444839478\nEpoch 206, Loss: 0.5227568745613098\nEpoch 207, Loss: 0.522686243057251\nEpoch 208, Loss: 0.5226162075996399\nEpoch 209, Loss: 0.5225464701652527\nEpoch 210, Loss: 0.5224773287773132\nEpoch 211, Loss: 0.5224083662033081\nEpoch 212, Loss: 0.5223400592803955\nEpoch 213, Loss: 0.522271990776062\nEpoch 214, Loss: 0.5222042798995972\nEpoch 215, Loss: 0.5221371054649353\nEpoch 216, Loss: 0.5220702290534973\nEpoch 217, Loss: 0.5220038294792175\nEpoch 218, Loss: 0.5219377875328064\nEpoch 219, Loss: 0.5218721032142639\nEpoch 220, Loss: 0.5218068361282349\nEpoch 221, Loss: 0.5217419266700745\nEpoch 222, Loss: 0.5216773152351379\nEpoch 223, Loss: 0.5216131210327148\nEpoch 224, Loss: 0.5215493440628052\nEpoch 225, Loss: 0.5214859247207642\nEpoch 226, Loss: 0.521422803401947\nEpoch 227, Loss: 0.5213600397109985\nEpoch 228, Loss: 0.5212976932525635\nEpoch 229, Loss: 0.5212356448173523\nEpoch 230, Loss: 0.5211740136146545\nEpoch 231, Loss: 0.5211126208305359\nEpoch 232, Loss: 0.5210517048835754\nEpoch 233, Loss: 0.5209910869598389\nEpoch 234, Loss: 0.5209307074546814\nEpoch 235, Loss: 0.5208708047866821\nEpoch 236, Loss: 0.520811140537262\nEpoch 237, Loss: 0.5207518339157104\nEpoch 238, Loss: 0.5206928253173828\nEpoch 239, Loss: 0.5206341743469238\nEpoch 240, Loss: 0.5205758810043335\nEpoch 241, Loss: 0.520517885684967\nEpoch 242, Loss: 0.5204602479934692\nEpoch 243, Loss: 0.5204027891159058\nEpoch 244, Loss: 0.5203457474708557\nEpoch 245, Loss: 0.5202890038490295\nEpoch 246, Loss: 0.520232617855072\nEpoch 247, Loss: 0.5201764106750488\nEpoch 248, Loss: 0.5201206207275391\nEpoch 249, Loss: 0.5200650691986084\nEpoch 250, Loss: 0.5200097560882568\nEpoch 251, Loss: 0.5199548602104187\nEpoch 252, Loss: 0.5199002027511597\nEpoch 253, Loss: 0.5198459029197693\nEpoch 254, Loss: 0.519791841506958\nEpoch 255, Loss: 0.5197381377220154\nEpoch 256, Loss: 0.5196845531463623\nEpoch 257, Loss: 0.5196314454078674\nEpoch 258, Loss: 0.5195784568786621\nEpoch 259, Loss: 0.5195257663726807\nEpoch 260, Loss: 0.5194734334945679\nEpoch 261, Loss: 0.5194213390350342\nEpoch 262, Loss: 0.5193694829940796\nEpoch 263, Loss: 0.5193180441856384\nEpoch 264, Loss: 0.5192667245864868\nEpoch 265, Loss: 0.5192157030105591\nEpoch 266, Loss: 0.5191649794578552\nEpoch 267, Loss: 0.5191145539283752\nEpoch 268, Loss: 0.5190643668174744\nEpoch 269, Loss: 0.5190143585205078\nEpoch 270, Loss: 0.5189646482467651\nEpoch 271, Loss: 0.5189152359962463\nEpoch 272, Loss: 0.5188660621643066\nEpoch 273, Loss: 0.5188170671463013\nEpoch 274, Loss: 0.5187683701515198\nEpoch 275, Loss: 0.5187200307846069\nEpoch 276, Loss: 0.5186718106269836\nEpoch 277, Loss: 0.5186237692832947\nEpoch 278, Loss: 0.5185761451721191\nEpoch 279, Loss: 0.5185286998748779\nEpoch 280, Loss: 0.5184814929962158\nEpoch 281, Loss: 0.5184345841407776\nEpoch 282, Loss: 0.5183877944946289\nEpoch 283, Loss: 0.5183413624763489\nEpoch 284, Loss: 0.5182951092720032\nEpoch 285, Loss: 0.5182490944862366\nEpoch 286, Loss: 0.5182032585144043\nEpoch 287, Loss: 0.5181577205657959\nEpoch 288, Loss: 0.5181123614311218\nEpoch 289, Loss: 0.5180671811103821\nEpoch 290, Loss: 0.5180224180221558\nEpoch 291, Loss: 0.517977774143219\nEpoch 292, Loss: 0.5179333090782166\nEpoch 293, Loss: 0.517889142036438\nEpoch 294, Loss: 0.5178452134132385\nEpoch 295, Loss: 0.5178013443946838\nEpoch 296, Loss: 0.517757773399353\nEpoch 297, Loss: 0.5177143812179565\nEpoch 298, Loss: 0.5176712870597839\nEpoch 299, Loss: 0.5176284313201904\nEpoch 300, Loss: 0.5175856947898865\nAccuracy:  0.536\nPrecision:  0.5609756097560976\nRecall:  0.536\nAUROC:  0.536\n","output_type":"stream"}]},{"cell_type":"code","source":"# Test the classifier\nclassifier.eval()\n\ndf_image_test = pd.read_json(\"/kaggle/input/dev-seen-memes/dev_seen.jsonl\", lines=True)\n\nX_test = df_image_test['text']\ny_test = df_image_test['label']\n\ntest_embeddings = model.encode(X_test)\n\ntest_embeddings = torch.tensor(test_embeddings)\ntest_labels = torch.tensor(y_test)\n\nwith torch.no_grad():\n    outputs = classifier(test_embeddings)\n    _, predicted = torch.max(outputs, 1)\n\n# Compute precision, recall, and AUROC\nprecision = precision_score(test_labels, predicted, average='weighted')\nrecall = recall_score(test_labels, predicted, average='weighted')\nroc_auc = roc_auc_score(test_labels, predicted)\n\nprint('Accuracy: ', (predicted == test_labels).sum().item() / test_labels.size(0))\nprint('Precision: ', precision)\nprint('Recall: ', recall)\nprint('AUROC: ', roc_auc)\nall_predictions_sentence_transformer = predicted","metadata":{"execution":{"iopub.status.busy":"2023-06-16T05:40:48.049036Z","iopub.execute_input":"2023-06-16T05:40:48.049731Z","iopub.status.idle":"2023-06-16T05:40:48.567036Z","shell.execute_reply.started":"2023-06-16T05:40:48.049697Z","shell.execute_reply":"2023-06-16T05:40:48.566112Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efb30656e1434956a56ba663dd3cc6f7"}},"metadata":{}},{"name":"stdout","text":"Accuracy:  0.538\nPrecision:  0.5574867208672086\nRecall:  0.538\nAUROC:  0.5341649197484437\n","output_type":"stream"}]},{"cell_type":"code","source":"# !pip install torch\n# !pip install transformers\n# !pip install sentence-transformers","metadata":{"execution":{"iopub.status.busy":"2023-06-16T05:33:34.491682Z","iopub.execute_input":"2023-06-16T05:33:34.492058Z","iopub.status.idle":"2023-06-16T05:34:10.954026Z","shell.execute_reply.started":"2023-06-16T05:33:34.492028Z","shell.execute_reply":"2023-06-16T05:34:10.952880Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.29.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.14.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (5.4.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.5.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.28.2)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting sentence-transformers\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.29.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.64.1)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.0.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.15.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.23.5)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.10.1)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.1.99)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.14.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.5.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (5.4.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.5.5)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->sentence-transformers) (1.16.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence-transformers) (9.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.5.7)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\nBuilding wheels for collected packages: sentence-transformers\n  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=3a4f8096c6af711e9fe26c5ecdf3a4e412813ecd592616dae3536e84dd2a90ff\n  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\nSuccessfully built sentence-transformers\nInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-2.2.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"## Late fusion Code for Testing\n# print(all_predictions_vit)\n# print(all_predictions_sentence_transformer)\n### Code for testing\ndf_image_test = pd.read_json(\"/kaggle/input/dev-seen-memes/dev_seen.jsonl\", lines=True)\n\nX_test = df_image_test['text']\ny_test = df_image_test['label']\n\ntest_labels = torch.tensor(y_test)\naverage = [(x + y) / 2 for x, y in zip(all_predictions_vit, all_predictions_sentence_transformer)]\nrounded_average = [torch.round(value) for value in average]\npredicted = torch.tensor(rounded_average)\n# Compute precision, recall, and AUROC\nprecision = precision_score(test_labels, predicted, average='weighted')\nrecall = recall_score(test_labels, predicted, average='weighted')\nroc_auc = roc_auc_score(test_labels, predicted)\n\nprint('Accuracy: ', (predicted == test_labels).sum().item() / test_labels.size(0))\nprint('Precision: ', precision)\nprint('Recall: ', recall)\nprint('AUROC: ', roc_auc)","metadata":{"execution":{"iopub.status.busy":"2023-06-16T05:48:11.003995Z","iopub.execute_input":"2023-06-16T05:48:11.004363Z","iopub.status.idle":"2023-06-16T05:48:11.042466Z","shell.execute_reply.started":"2023-06-16T05:48:11.004332Z","shell.execute_reply":"2023-06-16T05:48:11.041355Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Accuracy:  0.526\nPrecision:  0.5909460992907801\nRecall:  0.526\nAUROC:  0.5207229841097119\n","output_type":"stream"}]},{"cell_type":"code","source":"### Code for validation\n## Late fusion\n# print(all_predictions_vit)\n# print(all_predictions_sentence_transformer)\n\ndf_image_test = pd.read_json(\"/kaggle/input/facebook-hateful-meme-dataset/data/dev.jsonl\", lines=True)\n\nX_test = df_image_test['text']\ny_test = df_image_test['label']\n\ntest_labels = torch.tensor(y_test)\naverage = [(x + y) / 2 for x, y in zip(all_predictions_vit, all_predictions_sentence_transformer)]\nrounded_average = [torch.round(value) for value in average]\npredicted = torch.tensor(rounded_average)\n# Compute precision, recall, and AUROC\nprecision = precision_score(test_labels, predicted, average='weighted')\nrecall = recall_score(test_labels, predicted, average='weighted')\nroc_auc = roc_auc_score(test_labels, predicted)\n\nprint('Accuracy: ', (predicted == test_labels).sum().item() / test_labels.size(0))\nprint('Precision: ', precision)\nprint('Recall: ', recall)\nprint('AUROC: ', roc_auc)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-16T05:48:52.237739Z","iopub.execute_input":"2023-06-16T05:48:52.238145Z","iopub.status.idle":"2023-06-16T05:48:52.296027Z","shell.execute_reply.started":"2023-06-16T05:48:52.238114Z","shell.execute_reply":"2023-06-16T05:48:52.295202Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Accuracy:  0.52\nPrecision:  0.5886524822695035\nRecall:  0.52\nAUROC:  0.52\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}