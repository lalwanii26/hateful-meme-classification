{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc7e9005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# !pip install transformers\n",
    "# !pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58d6cc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# f = pd.read_json(file_path, lines=True)\n",
    "df_image_train = pd.read_json(\"data/train.jsonl\", lines=True)\n",
    "df_image_val = pd.read_json(\"data/dev.jsonl\", lines=True)\n",
    "df_image_test = pd.read_json(\"data/dev_seen.jsonl\", lines=True)\n",
    "# df_image_val.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dc19b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b925783cf7e405a8a99159482de3c0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e9125/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9de5632317943c5b1d2a93f9b900872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245be227455f4457a68720527b9d0b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7e55de9125/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2d68b9158cc4570befc2a8f10055248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)55de9125/config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d09ea3a92e548a4b1aa2c0ac8caaa84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "378dfb51ff154a44af2bdf2e57897426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)125/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb8c2bb0ea4426c972edfccc634a176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2de0e5a00a114a6a8d3d7740ba7cd345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e71c44d213b549009d806b3225ffaec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f91c130ee734a3e862f14e0fb1304eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e9125/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b839df0e4b48ea867b612ee542e621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb2070bf0ea645288df3c883f3c7350d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)9125/train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e27ff90ccb42669d4479989a5f38cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7e55de9125/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "743e78d47d9241cdad8477408c0ccc0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)5de9125/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: This framework generates embeddings for each input sentence\n",
      "Embedding: [-1.37173804e-02 -4.28515561e-02 -1.56286024e-02  1.40537508e-02\n",
      "  3.95537391e-02  1.21796280e-01  2.94333510e-02 -3.17524113e-02\n",
      "  3.54960039e-02 -7.93140307e-02  1.75878033e-02 -4.04369645e-02\n",
      "  4.97259721e-02  2.54912451e-02 -7.18699768e-02  8.14968497e-02\n",
      "  1.47072645e-03  4.79627475e-02 -4.50335816e-02 -9.92174968e-02\n",
      " -2.81769224e-02  6.45045936e-02  4.44670171e-02 -4.76217456e-02\n",
      " -3.52952406e-02  4.38671671e-02 -5.28566018e-02  4.33030014e-04\n",
      "  1.01921484e-01  1.64072271e-02  3.26996669e-02 -3.45986858e-02\n",
      "  1.21339718e-02  7.94871300e-02  4.58339276e-03  1.57778822e-02\n",
      " -9.68210492e-03  2.87626311e-02 -5.05806357e-02 -1.55793801e-02\n",
      " -2.87907030e-02 -9.62281227e-03  3.15556228e-02  2.27349456e-02\n",
      "  8.71449560e-02 -3.85027453e-02 -8.84718671e-02 -8.75497982e-03\n",
      " -2.12343298e-02  2.08924189e-02 -9.02078301e-02 -5.25732413e-02\n",
      " -1.05638728e-02  2.88311131e-02 -1.61454789e-02  6.17839675e-03\n",
      " -1.23234186e-02 -1.07337004e-02  2.83353440e-02 -5.28567955e-02\n",
      " -3.58618014e-02 -5.97989075e-02 -1.09055005e-02  2.91566513e-02\n",
      "  7.97979310e-02 -3.27905931e-04  6.83496427e-03  1.32718654e-02\n",
      " -4.24619988e-02  1.87656879e-02 -9.89234373e-02  2.09050123e-02\n",
      " -8.69605988e-02 -1.50152054e-02 -4.86202352e-02  8.04414824e-02\n",
      " -3.67701426e-03 -6.65044487e-02  1.14556760e-01 -3.04228310e-02\n",
      "  2.96631362e-02 -2.80694906e-02  4.64990437e-02 -2.25513969e-02\n",
      "  8.54223222e-02  3.15446742e-02  7.34541938e-02 -2.21861750e-02\n",
      " -5.29679097e-02  1.27130458e-02 -5.27339950e-02 -1.06188767e-01\n",
      "  7.04731569e-02  2.76736394e-02 -8.05530995e-02  2.39649713e-02\n",
      " -2.65124906e-02 -2.17331313e-02  4.35275324e-02  4.84711938e-02\n",
      " -2.37066932e-02  2.85768751e-02  1.11846142e-01 -6.34936243e-02\n",
      " -1.58318672e-02 -2.26169880e-02 -1.31028201e-02 -1.62071199e-03\n",
      " -3.60928923e-02 -9.78297442e-02 -4.67729531e-02  1.76272057e-02\n",
      " -3.97492275e-02 -1.76426329e-04  3.39627750e-02 -2.09633894e-02\n",
      "  6.33659121e-03 -2.59411559e-02  8.10410753e-02  6.14393130e-02\n",
      " -5.44600701e-03  6.48275986e-02 -1.16844065e-01  2.36860942e-02\n",
      " -1.32058552e-02 -1.12476453e-01  1.90049466e-02 -1.74660599e-34\n",
      "  5.58949672e-02  1.94244403e-02  4.65438776e-02  5.18645607e-02\n",
      "  3.89390141e-02  3.40541005e-02 -4.32114415e-02  7.90637434e-02\n",
      " -9.79530364e-02 -1.27441250e-02 -2.91870795e-02  1.02052316e-02\n",
      "  1.88115910e-02  1.08942553e-01  6.63464963e-02 -5.35295159e-02\n",
      " -3.29229012e-02  4.69826721e-02  2.28883103e-02  2.74114646e-02\n",
      " -2.91983485e-02  3.12706344e-02 -2.22850814e-02 -1.02282144e-01\n",
      " -2.79116798e-02  1.13793286e-02  9.06309038e-02 -4.75414544e-02\n",
      " -1.00718938e-01 -1.23232119e-02 -7.96928406e-02 -1.44636529e-02\n",
      " -7.76400566e-02 -7.66920485e-03  9.73955542e-03  2.24204753e-02\n",
      "  7.77267888e-02 -3.17155849e-03  2.11538505e-02 -3.30393985e-02\n",
      "  9.55248810e-03 -3.73012014e-02  2.61360351e-02 -9.79083031e-03\n",
      " -6.31505325e-02  5.77436853e-03 -3.80031578e-02  1.29684182e-02\n",
      " -1.82498898e-02 -1.56282950e-02 -1.23359286e-03  5.55579402e-02\n",
      "  1.13112706e-04 -5.61256818e-02  7.40165859e-02  1.84452068e-02\n",
      " -2.66368333e-02  1.31951850e-02  7.50086606e-02 -2.46797539e-02\n",
      " -3.24006192e-02 -1.57674737e-02 -8.03513546e-03 -5.61318407e-03\n",
      "  1.05687594e-02  3.26173822e-03 -3.91990319e-02 -9.38677043e-02\n",
      "  1.14227146e-01  6.57304749e-02 -4.72633094e-02  1.45087829e-02\n",
      " -3.54490504e-02 -3.37761641e-02 -5.15506119e-02 -3.80999781e-03\n",
      " -5.15036210e-02 -5.93429990e-02 -1.69410568e-03  7.42107481e-02\n",
      " -4.20091376e-02 -7.19975159e-02  3.17250155e-02 -1.66303609e-02\n",
      "  3.96984816e-03 -6.52750805e-02  2.77391262e-02 -7.51649961e-02\n",
      "  2.27456149e-02 -3.91368233e-02  1.54315867e-02 -5.54908514e-02\n",
      "  1.23318192e-02 -2.59520598e-02  6.66423514e-02 -6.91259509e-34\n",
      "  3.31628695e-02  8.47928822e-02 -6.65584058e-02  3.33541557e-02\n",
      "  4.71608667e-03  1.35361804e-02 -5.38694337e-02  9.20694023e-02\n",
      " -2.96876617e-02  3.16219740e-02 -2.37497669e-02  1.98771041e-02\n",
      "  1.03446163e-01 -9.06947404e-02  6.30628876e-03  1.42886238e-02\n",
      "  1.19293500e-02  6.43731048e-03  4.20104563e-02  1.25344954e-02\n",
      "  3.93019393e-02  5.35691381e-02 -4.30749841e-02  6.10432848e-02\n",
      " -5.39360553e-05  6.91682622e-02  1.05520561e-02  1.22111533e-02\n",
      " -7.23185614e-02  2.50469148e-02 -5.18370941e-02 -4.36562598e-02\n",
      " -6.71818182e-02  1.34828128e-02 -7.25888833e-02  7.04162661e-03\n",
      "  6.58939332e-02  1.08993854e-02 -2.60010920e-03  5.49969077e-02\n",
      "  5.06967194e-02  3.27948555e-02 -6.68833032e-02  6.45557120e-02\n",
      " -2.52076536e-02 -2.92572007e-02 -1.16696760e-01  3.24064456e-02\n",
      "  5.85858710e-02 -3.51756327e-02 -7.15240017e-02  2.24935990e-02\n",
      " -1.00786731e-01 -4.74544801e-02 -7.61962533e-02 -5.87166809e-02\n",
      "  4.21138369e-02 -7.47213736e-02  1.98468417e-02 -3.36504751e-03\n",
      " -5.29736690e-02  2.74728984e-02  3.45736742e-02 -6.11846857e-02\n",
      "  1.06364779e-01 -9.64120105e-02 -4.55944985e-02  1.51490094e-02\n",
      " -5.13530383e-03 -6.64447546e-02  4.31721509e-02 -1.10405590e-02\n",
      " -9.80253890e-03  7.53783211e-02 -1.49570992e-02 -4.80208807e-02\n",
      "  5.80726415e-02 -2.43896618e-02 -2.23138109e-02 -4.36992832e-02\n",
      "  5.12054116e-02 -3.28625739e-02  1.08763322e-01  6.08926043e-02\n",
      "  3.30793415e-03  5.53820319e-02  8.43200833e-02  1.27087301e-02\n",
      "  3.84465680e-02  6.52326047e-02 -2.94683855e-02  5.08005582e-02\n",
      " -2.09348276e-02  1.46135673e-01  2.25561652e-02 -1.77227779e-08\n",
      " -5.02672754e-02 -2.79213622e-04 -1.00328535e-01  2.42811311e-02\n",
      " -7.54043236e-02 -3.79139893e-02  3.96049805e-02  3.10079884e-02\n",
      " -9.05705430e-03 -6.50411844e-02  4.05453332e-02  4.83390428e-02\n",
      " -4.56962436e-02  4.76003485e-03  2.64365342e-03  9.35614556e-02\n",
      " -4.02599089e-02  3.27402167e-02  1.18298465e-02  5.54344654e-02\n",
      "  1.48052230e-01  7.21189529e-02  2.76969542e-04  1.68651622e-02\n",
      "  8.34880210e-03 -8.76153447e-03 -1.33649968e-02  6.14236519e-02\n",
      "  1.57168005e-02  6.94960877e-02  1.08621782e-02  6.08018450e-02\n",
      " -5.33421189e-02 -3.47924680e-02 -3.36272120e-02  6.93906993e-02\n",
      "  1.22987805e-02 -1.45237356e-01 -2.06969958e-03 -4.61132899e-02\n",
      "  3.72747425e-03 -5.59357274e-03 -1.00659840e-01 -4.45953235e-02\n",
      "  5.40921278e-02  4.98897117e-03  1.49534456e-02 -8.26059431e-02\n",
      "  6.26630336e-02 -5.01910783e-03 -4.81857844e-02 -3.53991315e-02\n",
      "  9.03388951e-03 -2.42337696e-02  5.66267595e-02  2.51528919e-02\n",
      " -1.70709323e-02 -1.24780117e-02  3.19518223e-02  1.38421198e-02\n",
      " -1.55814681e-02  1.00178316e-01  1.23657271e-01 -4.22967076e-02]\n",
      "\n",
      "Sentence: Sentences are passed as a list of string.\n",
      "Embedding: [ 5.64524755e-02  5.50023913e-02  3.13795693e-02  3.39485072e-02\n",
      " -3.54247205e-02  8.34667683e-02  9.88800749e-02  7.27546960e-03\n",
      " -6.68660412e-03 -7.65808113e-03  7.93738663e-02  7.39702606e-04\n",
      "  1.49291754e-02 -1.51046673e-02  3.67674418e-02  4.78743315e-02\n",
      " -4.81969677e-02 -3.76052633e-02 -4.60277908e-02 -8.89816210e-02\n",
      "  1.20228164e-01  1.30663306e-01 -3.73936221e-02  2.47855787e-03\n",
      "  2.55824765e-03  7.25814924e-02 -6.80436492e-02 -5.24695888e-02\n",
      "  4.90234159e-02  2.99563445e-02 -5.84429689e-02 -2.02263109e-02\n",
      "  2.08821949e-02  9.76691917e-02  3.52390781e-02  3.91140878e-02\n",
      "  1.05667925e-02  1.56231888e-03 -1.30822640e-02  8.52904934e-03\n",
      " -4.84092813e-03 -2.03766655e-02 -2.71800794e-02  2.83307694e-02\n",
      "  3.66017744e-02  2.51276158e-02 -9.90861952e-02  1.15626408e-02\n",
      " -3.60380597e-02 -7.23783895e-02 -1.12670109e-01  1.12942290e-02\n",
      " -3.86397541e-02  4.67386059e-02 -2.88460609e-02  2.26704124e-02\n",
      " -8.52403883e-03  3.32814865e-02 -1.06581894e-03 -7.09744841e-02\n",
      " -6.31170198e-02 -5.72186671e-02 -6.16026297e-02  5.47146797e-02\n",
      "  1.18317986e-02 -4.66261394e-02  2.56960150e-02 -7.07413489e-03\n",
      " -5.73842935e-02  4.12839092e-02 -5.91503568e-02  5.89021631e-02\n",
      " -4.41697873e-02  4.65081483e-02 -3.15814503e-02  5.58312461e-02\n",
      "  5.54578640e-02 -5.96533269e-02  4.06407751e-02  4.83765732e-03\n",
      " -4.96768467e-02 -1.00944333e-01  3.40078361e-02  4.13274858e-03\n",
      " -2.93530780e-03  2.11837646e-02 -3.73962224e-02 -2.79067028e-02\n",
      " -4.61768061e-02  5.26138805e-02 -2.79734898e-02 -1.62379295e-01\n",
      "  6.61042556e-02  1.72274541e-02 -5.45113115e-03  4.74474095e-02\n",
      " -3.82237658e-02 -3.96896526e-02  1.34545024e-02  4.49653715e-02\n",
      "  4.53670416e-03  2.82978285e-02  8.36632997e-02 -1.00857820e-02\n",
      " -1.19354032e-01 -3.84624302e-02  4.82859015e-02 -9.46083814e-02\n",
      "  1.91854369e-02 -9.96518657e-02 -6.30596727e-02  3.02696358e-02\n",
      "  1.17402431e-02 -4.78372648e-02 -6.20274572e-03 -3.32850814e-02\n",
      " -4.04390600e-03  1.28307231e-02  4.05254923e-02  7.56477043e-02\n",
      "  2.92434990e-02  2.84270141e-02 -2.78938506e-02  1.66857801e-02\n",
      " -2.47961748e-02 -6.83651119e-02  2.89968420e-02 -5.39867858e-33\n",
      " -2.69013946e-03 -2.65068747e-02 -6.47865934e-04 -8.46206304e-03\n",
      " -7.35154673e-02  4.94081806e-03 -5.97842447e-02  1.03438338e-02\n",
      "  2.12902040e-03 -2.88213091e-03 -3.17076743e-02 -9.42363888e-02\n",
      "  3.03019769e-02  7.00226873e-02  4.50685397e-02  3.69439349e-02\n",
      "  1.13594104e-02  3.53027284e-02  5.50449546e-03  1.34416251e-03\n",
      "  3.46119562e-03  7.75048062e-02  5.45112565e-02 -7.92055950e-02\n",
      " -9.31696668e-02 -4.03398424e-02  3.10668536e-02 -3.83081585e-02\n",
      " -5.89442700e-02  1.93331949e-02 -2.67159808e-02 -7.91938528e-02\n",
      "  1.04231112e-04  7.70621076e-02  4.16604020e-02  8.90932605e-02\n",
      "  3.56843472e-02 -1.09152915e-02  3.71498354e-02 -2.07070690e-02\n",
      " -2.46100686e-02 -2.05025803e-02  2.62201745e-02  3.43590491e-02\n",
      "  4.39251065e-02 -8.20518192e-03 -8.40710700e-02  4.24170904e-02\n",
      "  4.87498567e-02  5.95384650e-02  2.87747774e-02  3.37638482e-02\n",
      " -4.07442749e-02 -1.66368694e-03  7.91927427e-02  3.41088548e-02\n",
      " -5.72819961e-04  1.87749676e-02 -1.36964135e-02  7.38333240e-02\n",
      "  5.74428937e-04  8.33505243e-02  5.60811087e-02 -1.13711124e-02\n",
      "  4.42611314e-02  2.69581825e-02 -4.80535813e-02 -3.15087438e-02\n",
      "  7.75225908e-02  1.81773622e-02 -8.83005261e-02 -7.85518996e-03\n",
      " -6.22243099e-02  7.19372481e-02 -2.33475156e-02  6.52482780e-03\n",
      " -9.49527696e-03 -9.88312811e-02  4.01306041e-02  3.07396706e-02\n",
      " -2.21607238e-02 -9.45911556e-02  1.02367802e-02  1.02187745e-01\n",
      " -4.12960127e-02 -3.15777846e-02  4.74751852e-02 -1.10209838e-01\n",
      "  1.69615094e-02 -3.71709205e-02 -1.03262113e-02 -4.72538583e-02\n",
      " -1.20214568e-02 -1.93255134e-02  5.79292215e-02  4.23865952e-34\n",
      "  3.92013304e-02  8.41361359e-02 -1.02946743e-01  6.92259446e-02\n",
      "  1.68821365e-02 -3.26760784e-02  9.65962186e-03  1.80899650e-02\n",
      "  2.17939764e-02  1.63189098e-02 -9.69292223e-02  3.74852214e-03\n",
      " -2.38457359e-02 -3.44055854e-02  7.11962655e-02  9.21939733e-04\n",
      " -6.23862864e-03  3.23754400e-02 -8.90380412e-04  5.01906639e-03\n",
      " -4.24537808e-02  9.89083871e-02 -4.60321084e-02  4.69704941e-02\n",
      " -1.75283868e-02 -7.02518038e-03  1.32743921e-02 -5.30152135e-02\n",
      "  2.66402611e-03  1.45819252e-02  7.43345823e-03 -3.07132043e-02\n",
      " -2.09416524e-02  8.24109986e-02 -5.15894629e-02 -2.71178279e-02\n",
      "  1.17582999e-01  7.72503624e-03 -1.89522766e-02  3.94559279e-02\n",
      "  7.17360452e-02  2.59117540e-02  2.75191702e-02  9.50542279e-03\n",
      " -3.02355196e-02 -4.07944471e-02 -1.04028471e-01 -7.97418784e-03\n",
      " -3.64453555e-03  3.29716168e-02 -2.35954411e-02 -7.50518031e-03\n",
      " -5.82233779e-02 -3.17906104e-02 -4.18049060e-02  2.17453465e-02\n",
      " -6.67292327e-02 -4.89104278e-02  4.58515761e-03 -2.66046673e-02\n",
      " -1.12597056e-01  5.11167012e-02  5.48534207e-02 -6.69856817e-02\n",
      "  1.26766279e-01 -8.59488025e-02 -5.94231263e-02 -2.92189000e-03\n",
      " -1.14875827e-02 -1.26025900e-01 -3.48280719e-03 -9.12001655e-02\n",
      " -1.22933082e-01  1.33777643e-02 -4.75775450e-02 -6.57933205e-02\n",
      " -3.39410156e-02 -3.07107884e-02 -5.22033907e-02 -2.35463642e-02\n",
      "  5.90035319e-02 -3.85757834e-02  3.19701284e-02  4.05118614e-02\n",
      "  1.67077780e-02 -3.58281508e-02  1.45687955e-02  3.20138074e-02\n",
      " -1.34843737e-02  6.07819781e-02 -8.31401069e-03 -1.08105782e-02\n",
      "  4.69410606e-02  7.66134113e-02 -4.23400067e-02 -2.11963371e-08\n",
      " -7.25292861e-02 -4.20227908e-02 -6.12374544e-02  5.24666086e-02\n",
      " -1.42363524e-02  1.18487412e-02 -1.40788965e-02 -3.67529988e-02\n",
      " -4.44977246e-02 -1.15140863e-02  5.23316935e-02  2.96652149e-02\n",
      " -4.62781116e-02 -3.70892771e-02  1.89129710e-02  2.04307586e-02\n",
      " -2.24005990e-02 -1.48562761e-02 -1.79503970e-02  4.20007892e-02\n",
      "  1.40942410e-02 -2.83492785e-02 -1.16863027e-01  1.48957046e-02\n",
      " -7.30604690e-04  5.66028021e-02 -2.68740430e-02  1.09106675e-01\n",
      "  2.94566527e-03  1.19267911e-01  1.14212438e-01  8.92973691e-02\n",
      " -1.70255415e-02 -4.99054156e-02 -2.11930890e-02  3.18421572e-02\n",
      "  7.03436211e-02 -1.02929451e-01  8.23816806e-02  2.81968210e-02\n",
      "  3.21146511e-02  3.79108377e-02 -1.09553121e-01  8.19620341e-02\n",
      "  8.73216316e-02 -5.73564023e-02 -2.01708917e-02 -5.69444075e-02\n",
      " -1.30338352e-02 -5.55684865e-02 -1.32966489e-02  8.64007510e-03\n",
      "  5.30011877e-02 -4.06847224e-02  2.71709133e-02 -2.55948585e-03\n",
      "  3.05775534e-02 -4.61865179e-02  4.68033599e-03 -3.64947021e-02\n",
      "  6.80802613e-02  6.65087625e-02  8.49152356e-02 -3.32849249e-02]\n",
      "\n",
      "Sentence: The quick brown fox jumps over the lazy dog.\n",
      "Embedding: [ 4.39335518e-02  5.89343682e-02  4.81783859e-02  7.75481090e-02\n",
      "  2.67444104e-02 -3.76295969e-02 -2.60512927e-03 -5.99430203e-02\n",
      " -2.49602692e-03  2.20728144e-02  4.80259582e-02  5.57553023e-02\n",
      " -3.89454141e-02 -2.66168024e-02  7.69342063e-03 -2.62376387e-02\n",
      " -3.64160724e-02 -3.78161296e-02  7.40781575e-02 -4.95050512e-02\n",
      " -5.85217327e-02 -6.36197180e-02  3.24349888e-02  2.20085196e-02\n",
      " -7.10637718e-02 -3.31577845e-02 -6.94104135e-02 -5.00374548e-02\n",
      "  7.46268034e-02 -1.11133814e-01 -1.23063410e-02  3.77456322e-02\n",
      " -2.80313175e-02  1.45353368e-02 -3.15585472e-02 -8.05836841e-02\n",
      "  5.83525896e-02  2.59008491e-03  3.92802581e-02  2.57696044e-02\n",
      "  4.98505533e-02 -1.75622769e-03 -4.55298051e-02  2.92607751e-02\n",
      " -1.02017276e-01  5.22287525e-02 -7.90899843e-02 -1.02857500e-02\n",
      "  9.20248125e-03  1.30732385e-02 -4.04777452e-02 -2.77924985e-02\n",
      "  1.24667538e-02  6.72832951e-02  6.81248382e-02 -7.57122086e-03\n",
      " -6.09947601e-03 -4.23777103e-02  5.17815948e-02 -1.56707130e-02\n",
      "  9.56359133e-03  4.12390418e-02  2.14959234e-02  1.04293460e-02\n",
      "  2.73349397e-02  1.87062416e-02 -2.69607175e-02 -7.00541958e-02\n",
      " -1.04700506e-01 -1.89876580e-03  1.77016854e-02 -5.74725494e-02\n",
      " -1.44223254e-02  4.70469240e-04  2.33226269e-03 -2.51920428e-02\n",
      "  4.93004173e-02 -5.09609580e-02  6.31982908e-02  1.49165383e-02\n",
      " -2.70766653e-02 -4.52875793e-02 -4.90593947e-02  3.74940932e-02\n",
      "  3.84579487e-02  1.56902603e-03  3.09922453e-02  2.01630387e-02\n",
      " -1.24363480e-02 -3.06719858e-02 -2.78819446e-02 -6.89182803e-02\n",
      " -5.13677746e-02  2.14795396e-02  1.15747098e-02  1.25406671e-03\n",
      "  1.88765880e-02 -4.42318805e-02 -4.49817143e-02 -3.41866608e-03\n",
      "  1.31131113e-02  2.00099256e-02  1.21099785e-01  2.31074803e-02\n",
      " -2.20160037e-02 -3.28846909e-02 -3.15513997e-03  1.17834585e-04\n",
      "  9.91498753e-02  1.65239032e-02 -4.69667464e-03 -1.45366890e-02\n",
      " -3.71075142e-03  9.65136290e-02  2.85908077e-02  2.13482101e-02\n",
      " -7.17644915e-02 -2.41142306e-02 -4.40940335e-02 -1.07346877e-01\n",
      "  6.79945499e-02  1.30466834e-01 -7.97029585e-02  6.79509714e-03\n",
      " -2.37511992e-02 -4.61636595e-02 -2.99650598e-02 -3.69410120e-33\n",
      "  7.30969831e-02 -2.20172182e-02 -8.61464441e-02 -7.14379326e-02\n",
      " -6.36741295e-02 -7.21863061e-02 -5.93044143e-03 -2.33641677e-02\n",
      " -2.83658262e-02  4.77434881e-02 -8.06176588e-02 -1.56481564e-03\n",
      "  1.38443764e-02 -2.86236182e-02 -3.35386992e-02 -1.13777541e-01\n",
      " -9.17633809e-03 -1.08101154e-02  3.23196203e-02  5.88380247e-02\n",
      "  3.34208943e-02  1.07987948e-01 -3.72713171e-02 -2.96770334e-02\n",
      "  5.17189652e-02 -2.25338917e-02 -6.96091205e-02 -2.14475542e-02\n",
      " -2.33410634e-02  4.82199825e-02 -3.58766466e-02 -4.68990915e-02\n",
      " -3.97873484e-02  1.10813268e-01 -1.43007403e-02 -1.18464477e-01\n",
      "  5.82915172e-02 -6.25889078e-02 -2.94040721e-02  6.03238232e-02\n",
      " -2.44414527e-03  1.60116218e-02  2.67233383e-02  2.49530710e-02\n",
      " -6.49319217e-02 -1.06802462e-02  2.81464625e-02  1.03563257e-02\n",
      " -6.63571351e-04  1.98186226e-02 -3.04288417e-02  6.28424250e-03\n",
      "  5.15268408e-02 -4.75375429e-02 -6.44421354e-02  9.55032408e-02\n",
      "  7.55858421e-02 -2.81574931e-02 -3.49966027e-02  1.01816408e-01\n",
      "  1.98732372e-02 -3.68036777e-02  2.93520140e-03 -5.00745587e-02\n",
      "  1.50932148e-01 -6.16079606e-02 -8.58812705e-02  7.13993423e-03\n",
      " -1.33065740e-02  7.80404732e-02  1.75250247e-02  4.21279483e-02\n",
      "  3.57940048e-02 -1.32950425e-01  3.56970169e-02 -2.03116778e-02\n",
      "  1.24909896e-02 -3.80355194e-02  4.91543338e-02 -1.56540908e-02\n",
      "  1.21418245e-01 -8.08644593e-02 -4.68781404e-02  4.10843007e-02\n",
      " -1.84317902e-02  6.69691414e-02  4.33595153e-03  2.27315426e-02\n",
      " -1.36429071e-02 -4.53238413e-02 -3.92829701e-02 -6.29891036e-03\n",
      "  5.29609695e-02 -3.69065106e-02  7.11677447e-02  2.33343269e-33\n",
      "  1.05231375e-01 -4.81874384e-02  6.95918798e-02  6.56976476e-02\n",
      " -4.65149470e-02  5.14492393e-02 -1.24475341e-02  3.20872292e-02\n",
      " -9.23356563e-02  5.00932746e-02 -3.28876488e-02  1.39139118e-02\n",
      " -8.70246615e-04 -4.90904460e-03  1.03946395e-01  3.21620930e-04\n",
      "  5.28110154e-02 -1.17990365e-02  2.31565107e-02  1.31767904e-02\n",
      " -5.25962897e-02  3.26702036e-02  3.08719609e-04  6.41129017e-02\n",
      "  3.88500988e-02  5.88008314e-02  8.29793364e-02 -1.88149232e-02\n",
      " -2.26377118e-02 -1.00473635e-01 -3.83752249e-02 -5.88081218e-02\n",
      "  1.82421715e-03 -4.26995233e-02  2.50195023e-02  6.40059933e-02\n",
      " -3.77483182e-02 -6.83900807e-03 -2.54605827e-03 -9.76042822e-02\n",
      "  1.88476015e-02 -8.83176457e-04  1.73611697e-02  7.10790232e-02\n",
      "  3.30393091e-02  6.93425816e-03 -5.60523383e-02  5.14634103e-02\n",
      " -4.29542214e-02  4.60076630e-02 -8.78833141e-03  3.17289308e-02\n",
      "  4.93965633e-02  2.95189712e-02 -5.05192466e-02 -5.43187149e-02\n",
      "  1.49964995e-04 -2.76614632e-02  3.46877985e-02 -2.10890137e-02\n",
      "  1.38060348e-02  2.99886838e-02  1.39744608e-02 -4.26469184e-03\n",
      " -1.50337163e-02 -8.76095220e-02 -6.85053766e-02 -4.28141728e-02\n",
      "  7.76944980e-02 -7.10285380e-02 -7.37691857e-03  2.13727262e-02\n",
      "  1.35562327e-02 -7.90464729e-02  5.47669642e-03  8.30663666e-02\n",
      "  1.14148043e-01  1.80764520e-03  8.75491425e-02 -4.16045226e-02\n",
      "  1.55416466e-02 -1.01206452e-02 -7.32437801e-03  1.07965842e-02\n",
      " -6.62816614e-02  3.98413911e-02 -1.16711587e-01  6.42993897e-02\n",
      "  4.02920060e-02 -6.54741526e-02  1.95052605e-02  8.09995681e-02\n",
      "  5.36463410e-02  7.67969638e-02 -1.34852324e-02 -1.76919084e-08\n",
      " -4.43935096e-02  9.20642167e-03 -8.79590288e-02  4.26921658e-02\n",
      "  7.31365010e-02  1.68427248e-02 -4.03263085e-02  1.85131468e-02\n",
      "  8.44172537e-02 -3.74477394e-02  3.02996244e-02  2.90641822e-02\n",
      "  6.36878833e-02  2.89750230e-02 -1.47269825e-02  1.77542828e-02\n",
      " -3.36895213e-02  1.73161216e-02  3.37875187e-02  1.76826075e-01\n",
      " -1.75533630e-02 -6.03077821e-02 -1.43394582e-02 -2.38536559e-02\n",
      " -4.45530191e-02 -2.89850570e-02 -8.96776691e-02 -1.75941200e-03\n",
      " -2.61485819e-02  5.93996700e-03 -5.18355444e-02  8.57279748e-02\n",
      " -8.18399116e-02  8.35441053e-03  4.00790386e-02  4.17764448e-02\n",
      "  1.04573578e-01 -2.86564953e-03  1.96691062e-02  5.81048708e-03\n",
      "  1.33253522e-02  4.51001078e-02 -2.17588302e-02 -1.39493020e-02\n",
      " -6.86992407e-02 -2.94111297e-03 -3.10765114e-02 -1.05854370e-01\n",
      "  6.91623986e-02 -4.24114726e-02 -4.67682257e-02 -3.64750735e-02\n",
      "  4.50399965e-02  6.09817058e-02 -6.56561926e-02 -5.45643736e-03\n",
      " -1.86226945e-02 -6.31484389e-02 -3.87436859e-02  3.46733704e-02\n",
      "  5.55458069e-02  5.21628074e-02  5.61065413e-02  1.02063939e-01]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "#Our sentences we like to encode\n",
    "sentences = ['This framework generates embeddings for each input sentence',\n",
    "    'Sentences are passed as a list of string.',\n",
    "    'The quick brown fox jumps over the lazy dog.']\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "#Print the embeddings\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4133dee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-16 04:45:10.527986: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-16 04:45:11.667220: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3889e2d4e779434ea2e386ac1a1d2fff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)821d1/.gitattributes:   0%|          | 0.00/391 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6475fe1b290413682eef570a3dfc2c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fced26e35f144d6c9d6f8eaf9418f6ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)8d01e821d1/README.md:   0%|          | 0.00/3.95k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceedd94d38c845bdb78f501821a47c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)d1/added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e42aa3edfdb4592b31126d0512c23f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)01e821d1/config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb38f4fa99fe4fb3a36cd42b806e7514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089f2a6ed09d4c5bbd25e461f4fa17c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8494b68a53a44afaaca392db44ce389c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b95d4890e614bab80e96e322048c9db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68cc500e22c8440b9a9a1bf77dd0d20e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)821d1/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f4a00778e545c7aa30a889833d8fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/399 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a03e1dfdd4d4a9d813e5889c38020df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)8d01e821d1/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e121f31bc47467c8fc7d09426cd95f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)1e821d1/modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/musharma/.local/lib/python3.9/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.695159912109375\n",
      "Epoch 2, Loss: 0.680543839931488\n",
      "Epoch 3, Loss: 0.6547991037368774\n",
      "Epoch 4, Loss: 0.6562486290931702\n",
      "Epoch 5, Loss: 0.6419801712036133\n",
      "Epoch 6, Loss: 0.6261335611343384\n",
      "Epoch 7, Loss: 0.62160724401474\n",
      "Epoch 8, Loss: 0.5995896458625793\n",
      "Epoch 9, Loss: 0.5805856585502625\n",
      "Epoch 10, Loss: 0.5945366621017456\n",
      "Epoch 11, Loss: 0.5812563896179199\n",
      "Epoch 12, Loss: 0.5955142378807068\n",
      "Epoch 13, Loss: 0.5800560712814331\n",
      "Epoch 14, Loss: 0.5738778710365295\n",
      "Epoch 15, Loss: 0.5736756324768066\n",
      "Epoch 16, Loss: 0.5637814402580261\n",
      "Epoch 17, Loss: 0.5725136995315552\n",
      "Epoch 18, Loss: 0.5644941329956055\n",
      "Epoch 19, Loss: 0.5625475645065308\n",
      "Epoch 20, Loss: 0.5624615550041199\n",
      "Epoch 21, Loss: 0.5566096305847168\n",
      "Epoch 22, Loss: 0.5610073804855347\n",
      "Epoch 23, Loss: 0.5573152899742126\n",
      "Epoch 24, Loss: 0.5548543334007263\n",
      "Epoch 25, Loss: 0.5550036430358887\n",
      "Epoch 26, Loss: 0.5505330562591553\n",
      "Epoch 27, Loss: 0.553121030330658\n",
      "Epoch 28, Loss: 0.5510523915290833\n",
      "Epoch 29, Loss: 0.549892246723175\n",
      "Epoch 30, Loss: 0.5492218136787415\n",
      "Epoch 31, Loss: 0.5465334057807922\n",
      "Epoch 32, Loss: 0.5479604005813599\n",
      "Epoch 33, Loss: 0.546118974685669\n",
      "Epoch 34, Loss: 0.5460561513900757\n",
      "Epoch 35, Loss: 0.5448124408721924\n",
      "Epoch 36, Loss: 0.5435627102851868\n",
      "Epoch 37, Loss: 0.5439373254776001\n",
      "Epoch 38, Loss: 0.5425402522087097\n",
      "Epoch 39, Loss: 0.5427208542823792\n",
      "Epoch 40, Loss: 0.5412460565567017\n",
      "Epoch 41, Loss: 0.5411120057106018\n",
      "Epoch 42, Loss: 0.5406095385551453\n",
      "Epoch 43, Loss: 0.5400830507278442\n",
      "Epoch 44, Loss: 0.5397687554359436\n",
      "Epoch 45, Loss: 0.5387821793556213\n",
      "Epoch 46, Loss: 0.538768470287323\n",
      "Epoch 47, Loss: 0.5380597114562988\n",
      "Epoch 48, Loss: 0.5380951166152954\n",
      "Epoch 49, Loss: 0.537337601184845\n",
      "Epoch 50, Loss: 0.537098228931427\n",
      "Epoch 51, Loss: 0.5365056991577148\n",
      "Epoch 52, Loss: 0.5362367033958435\n",
      "Epoch 53, Loss: 0.5358452200889587\n",
      "Epoch 54, Loss: 0.5355179905891418\n",
      "Epoch 55, Loss: 0.5352348685264587\n",
      "Epoch 56, Loss: 0.5348602533340454\n",
      "Epoch 57, Loss: 0.534588634967804\n",
      "Epoch 58, Loss: 0.5341561436653137\n",
      "Epoch 59, Loss: 0.5339038372039795\n",
      "Epoch 60, Loss: 0.5335196852684021\n",
      "Epoch 61, Loss: 0.5333328247070312\n",
      "Epoch 62, Loss: 0.5329914093017578\n",
      "Epoch 63, Loss: 0.5327872633934021\n",
      "Epoch 64, Loss: 0.5324192047119141\n",
      "Epoch 65, Loss: 0.5321878790855408\n",
      "Epoch 66, Loss: 0.5318610668182373\n",
      "Epoch 67, Loss: 0.5316661596298218\n",
      "Epoch 68, Loss: 0.5313732028007507\n",
      "Epoch 69, Loss: 0.531165361404419\n",
      "Epoch 70, Loss: 0.5308860540390015\n",
      "Epoch 71, Loss: 0.5306816697120667\n",
      "Epoch 72, Loss: 0.5304122567176819\n",
      "Epoch 73, Loss: 0.5301896333694458\n",
      "Epoch 74, Loss: 0.5299407839775085\n",
      "Epoch 75, Loss: 0.5297420620918274\n",
      "Epoch 76, Loss: 0.5295196175575256\n",
      "Epoch 77, Loss: 0.5293031334877014\n",
      "Epoch 78, Loss: 0.5290801525115967\n",
      "Epoch 79, Loss: 0.5288702845573425\n",
      "Epoch 80, Loss: 0.528669536113739\n",
      "Epoch 81, Loss: 0.5284566283226013\n",
      "Epoch 82, Loss: 0.5282625555992126\n",
      "Epoch 83, Loss: 0.5280603170394897\n",
      "Epoch 84, Loss: 0.5278785228729248\n",
      "Epoch 85, Loss: 0.5276736617088318\n",
      "Epoch 86, Loss: 0.527489960193634\n",
      "Epoch 87, Loss: 0.5272976756095886\n",
      "Epoch 88, Loss: 0.5271220803260803\n",
      "Epoch 89, Loss: 0.5269347429275513\n",
      "Epoch 90, Loss: 0.5267559289932251\n",
      "Epoch 91, Loss: 0.5265817046165466\n",
      "Epoch 92, Loss: 0.5264081358909607\n",
      "Epoch 93, Loss: 0.5262376070022583\n",
      "Epoch 94, Loss: 0.526062548160553\n",
      "Epoch 95, Loss: 0.5258995890617371\n",
      "Epoch 96, Loss: 0.5257322192192078\n",
      "Epoch 97, Loss: 0.5255708694458008\n",
      "Epoch 98, Loss: 0.5254082679748535\n",
      "Epoch 99, Loss: 0.5252506136894226\n",
      "Epoch 100, Loss: 0.5250946283340454\n",
      "Epoch 101, Loss: 0.524936854839325\n",
      "Epoch 102, Loss: 0.5247849225997925\n",
      "Epoch 103, Loss: 0.5246327519416809\n",
      "Epoch 104, Loss: 0.5244846940040588\n",
      "Epoch 105, Loss: 0.5243358016014099\n",
      "Epoch 106, Loss: 0.5241895914077759\n",
      "Epoch 107, Loss: 0.5240460634231567\n",
      "Epoch 108, Loss: 0.523902416229248\n",
      "Epoch 109, Loss: 0.5237618684768677\n",
      "Epoch 110, Loss: 0.5236220955848694\n",
      "Epoch 111, Loss: 0.5234847068786621\n",
      "Epoch 112, Loss: 0.5233485102653503\n",
      "Epoch 113, Loss: 0.5232129693031311\n",
      "Epoch 114, Loss: 0.5230801105499268\n",
      "Epoch 115, Loss: 0.5229480862617493\n",
      "Epoch 116, Loss: 0.522817850112915\n",
      "Epoch 117, Loss: 0.5226889252662659\n",
      "Epoch 118, Loss: 0.5225611925125122\n",
      "Epoch 119, Loss: 0.522435188293457\n",
      "Epoch 120, Loss: 0.5223100781440735\n",
      "Epoch 121, Loss: 0.5221866965293884\n",
      "Epoch 122, Loss: 0.5220646858215332\n",
      "Epoch 123, Loss: 0.5219436287879944\n",
      "Epoch 124, Loss: 0.521824061870575\n",
      "Epoch 125, Loss: 0.5217055678367615\n",
      "Epoch 126, Loss: 0.5215883851051331\n",
      "Epoch 127, Loss: 0.5214725732803345\n",
      "Epoch 128, Loss: 0.5213577151298523\n",
      "Epoch 129, Loss: 0.5212442278862\n",
      "Epoch 130, Loss: 0.5211318135261536\n",
      "Epoch 131, Loss: 0.5210205912590027\n",
      "Epoch 132, Loss: 0.5209105610847473\n",
      "Epoch 133, Loss: 0.5208014249801636\n",
      "Epoch 134, Loss: 0.5206934213638306\n",
      "Epoch 135, Loss: 0.5205867290496826\n",
      "Epoch 136, Loss: 0.5204808115959167\n",
      "Epoch 137, Loss: 0.5203761458396912\n",
      "Epoch 138, Loss: 0.520272433757782\n",
      "Epoch 139, Loss: 0.520169734954834\n",
      "Epoch 140, Loss: 0.5200680494308472\n",
      "Epoch 141, Loss: 0.5199674367904663\n",
      "Epoch 142, Loss: 0.5198677778244019\n",
      "Epoch 143, Loss: 0.5197689533233643\n",
      "Epoch 144, Loss: 0.5196713209152222\n",
      "Epoch 145, Loss: 0.5195744037628174\n",
      "Epoch 146, Loss: 0.5194784998893738\n",
      "Epoch 147, Loss: 0.5193835496902466\n",
      "Epoch 148, Loss: 0.519289493560791\n",
      "Epoch 149, Loss: 0.5191963315010071\n",
      "Epoch 150, Loss: 0.51910400390625\n",
      "Epoch 151, Loss: 0.5190125703811646\n",
      "Epoch 152, Loss: 0.5189220309257507\n",
      "Epoch 153, Loss: 0.5188324451446533\n",
      "Epoch 154, Loss: 0.5187435150146484\n",
      "Epoch 155, Loss: 0.51865553855896\n",
      "Epoch 156, Loss: 0.5185683369636536\n",
      "Epoch 157, Loss: 0.5184819102287292\n",
      "Epoch 158, Loss: 0.5183963179588318\n",
      "Epoch 159, Loss: 0.5183115601539612\n",
      "Epoch 160, Loss: 0.5182275176048279\n",
      "Epoch 161, Loss: 0.5181443095207214\n",
      "Epoch 162, Loss: 0.5180618762969971\n",
      "Epoch 163, Loss: 0.5179800391197205\n",
      "Epoch 164, Loss: 0.517899215221405\n",
      "Epoch 165, Loss: 0.5178189873695374\n",
      "Epoch 166, Loss: 0.5177394151687622\n",
      "Epoch 167, Loss: 0.5176606178283691\n",
      "Epoch 168, Loss: 0.5175825357437134\n",
      "Epoch 169, Loss: 0.5175052881240845\n",
      "Epoch 170, Loss: 0.5174285769462585\n",
      "Epoch 171, Loss: 0.5173525810241699\n",
      "Epoch 172, Loss: 0.5172773003578186\n",
      "Epoch 173, Loss: 0.5172026753425598\n",
      "Epoch 174, Loss: 0.5171286463737488\n",
      "Epoch 175, Loss: 0.5170553922653198\n",
      "Epoch 176, Loss: 0.5169827938079834\n",
      "Epoch 177, Loss: 0.51691073179245\n",
      "Epoch 178, Loss: 0.516839325428009\n",
      "Epoch 179, Loss: 0.5167685747146606\n",
      "Epoch 180, Loss: 0.5166984796524048\n",
      "Epoch 181, Loss: 0.5166289806365967\n",
      "Epoch 182, Loss: 0.5165600776672363\n",
      "Epoch 183, Loss: 0.5164917707443237\n",
      "Epoch 184, Loss: 0.5164240598678589\n",
      "Epoch 185, Loss: 0.5163569450378418\n",
      "Epoch 186, Loss: 0.5162904262542725\n",
      "Epoch 187, Loss: 0.5162245035171509\n",
      "Epoch 188, Loss: 0.5161591172218323\n",
      "Epoch 189, Loss: 0.5160942673683167\n",
      "Epoch 190, Loss: 0.516029953956604\n",
      "Epoch 191, Loss: 0.5159662961959839\n",
      "Epoch 192, Loss: 0.5159031748771667\n",
      "Epoch 193, Loss: 0.5158404111862183\n",
      "Epoch 194, Loss: 0.5157784223556519\n",
      "Epoch 195, Loss: 0.5157168507575989\n",
      "Epoch 196, Loss: 0.5156558156013489\n",
      "Epoch 197, Loss: 0.5155952572822571\n",
      "Epoch 198, Loss: 0.5155352354049683\n",
      "Epoch 199, Loss: 0.5154756903648376\n",
      "Epoch 200, Loss: 0.5154166221618652\n",
      "Epoch 201, Loss: 0.5153582692146301\n",
      "Epoch 202, Loss: 0.5153001546859741\n",
      "Epoch 203, Loss: 0.5152426958084106\n",
      "Epoch 204, Loss: 0.5151856541633606\n",
      "Epoch 205, Loss: 0.5151290893554688\n",
      "Epoch 206, Loss: 0.5150728225708008\n",
      "Epoch 207, Loss: 0.5150172114372253\n",
      "Epoch 208, Loss: 0.5149620175361633\n",
      "Epoch 209, Loss: 0.5149074196815491\n",
      "Epoch 210, Loss: 0.5148530602455139\n",
      "Epoch 211, Loss: 0.5147992372512817\n",
      "Epoch 212, Loss: 0.5147458910942078\n",
      "Epoch 213, Loss: 0.5146929621696472\n",
      "Epoch 214, Loss: 0.5146404504776001\n",
      "Epoch 215, Loss: 0.5145883560180664\n",
      "Epoch 216, Loss: 0.5145366787910461\n",
      "Epoch 217, Loss: 0.5144854187965393\n",
      "Epoch 218, Loss: 0.5144346356391907\n",
      "Epoch 219, Loss: 0.5143843293190002\n",
      "Epoch 220, Loss: 0.5143343210220337\n",
      "Epoch 221, Loss: 0.514284610748291\n",
      "Epoch 222, Loss: 0.5142354369163513\n",
      "Epoch 223, Loss: 0.514186680316925\n",
      "Epoch 224, Loss: 0.5141382217407227\n",
      "Epoch 225, Loss: 0.5140901803970337\n",
      "Epoch 226, Loss: 0.5140424966812134\n",
      "Epoch 227, Loss: 0.5139952898025513\n",
      "Epoch 228, Loss: 0.5139484405517578\n",
      "Epoch 229, Loss: 0.5139020085334778\n",
      "Epoch 230, Loss: 0.5138558149337769\n",
      "Epoch 231, Loss: 0.5138099789619446\n",
      "Epoch 232, Loss: 0.5137646198272705\n",
      "Epoch 233, Loss: 0.5137196183204651\n",
      "Epoch 234, Loss: 0.5136749148368835\n",
      "Epoch 235, Loss: 0.5136305093765259\n",
      "Epoch 236, Loss: 0.5135865211486816\n",
      "Epoch 237, Loss: 0.5135428309440613\n",
      "Epoch 238, Loss: 0.5134995579719543\n",
      "Epoch 239, Loss: 0.5134565830230713\n",
      "Epoch 240, Loss: 0.5134139657020569\n",
      "Epoch 241, Loss: 0.5133716464042664\n",
      "Epoch 242, Loss: 0.5133296847343445\n",
      "Epoch 243, Loss: 0.5132880210876465\n",
      "Epoch 244, Loss: 0.5132465958595276\n",
      "Epoch 245, Loss: 0.5132055878639221\n",
      "Epoch 246, Loss: 0.5131649374961853\n",
      "Epoch 247, Loss: 0.5131245255470276\n",
      "Epoch 248, Loss: 0.5130845308303833\n",
      "Epoch 249, Loss: 0.5130447745323181\n",
      "Epoch 250, Loss: 0.513005256652832\n",
      "Epoch 251, Loss: 0.5129660367965698\n",
      "Epoch 252, Loss: 0.5129272937774658\n",
      "Epoch 253, Loss: 0.5128885507583618\n",
      "Epoch 254, Loss: 0.512850284576416\n",
      "Epoch 255, Loss: 0.5128123164176941\n",
      "Epoch 256, Loss: 0.512774646282196\n",
      "Epoch 257, Loss: 0.5127372145652771\n",
      "Epoch 258, Loss: 0.512700080871582\n",
      "Epoch 259, Loss: 0.5126631855964661\n",
      "Epoch 260, Loss: 0.5126266479492188\n",
      "Epoch 261, Loss: 0.5125902891159058\n",
      "Epoch 262, Loss: 0.5125542879104614\n",
      "Epoch 263, Loss: 0.5125185251235962\n",
      "Epoch 264, Loss: 0.5124830007553101\n",
      "Epoch 265, Loss: 0.5124477744102478\n",
      "Epoch 266, Loss: 0.5124127864837646\n",
      "Epoch 267, Loss: 0.5123780369758606\n",
      "Epoch 268, Loss: 0.5123435854911804\n",
      "Epoch 269, Loss: 0.5123093724250793\n",
      "Epoch 270, Loss: 0.5122753381729126\n",
      "Epoch 271, Loss: 0.5122416615486145\n",
      "Epoch 272, Loss: 0.5122082233428955\n",
      "Epoch 273, Loss: 0.5121749639511108\n",
      "Epoch 274, Loss: 0.51214200258255\n",
      "Epoch 275, Loss: 0.5121092796325684\n",
      "Epoch 276, Loss: 0.512076735496521\n",
      "Epoch 277, Loss: 0.5120444297790527\n",
      "Epoch 278, Loss: 0.5120124220848083\n",
      "Epoch 279, Loss: 0.5119807124137878\n",
      "Epoch 280, Loss: 0.5119490027427673\n",
      "Epoch 281, Loss: 0.5119177103042603\n",
      "Epoch 282, Loss: 0.5118865966796875\n",
      "Epoch 283, Loss: 0.5118557214736938\n",
      "Epoch 284, Loss: 0.5118250250816345\n",
      "Epoch 285, Loss: 0.5117945671081543\n",
      "Epoch 286, Loss: 0.5117643475532532\n",
      "Epoch 287, Loss: 0.5117343068122864\n",
      "Epoch 288, Loss: 0.5117045640945435\n",
      "Epoch 289, Loss: 0.5116750001907349\n",
      "Epoch 290, Loss: 0.5116455554962158\n",
      "Epoch 291, Loss: 0.5116164684295654\n",
      "Epoch 292, Loss: 0.511587917804718\n",
      "Epoch 293, Loss: 0.5115597248077393\n",
      "Epoch 294, Loss: 0.5115326642990112\n",
      "Epoch 295, Loss: 0.5115078687667847\n",
      "Epoch 296, Loss: 0.5114884376525879\n",
      "Epoch 297, Loss: 0.5114820003509521\n",
      "Epoch 298, Loss: 0.5115082263946533\n",
      "Epoch 299, Loss: 0.5116152763366699\n",
      "Epoch 300, Loss: 0.5119376182556152\n",
      "Accuracy:  0.548\n",
      "Precision:  0.5690353460972016\n",
      "Recall:  0.548\n",
      "AUROC:  0.548\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "\n",
    "# Load the sentence transformer model\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "# Assume we have some data\n",
    "# texts = ['This is a good day!', 'I feel very sad', 'I am not happy about this', 'I am thrilled']\n",
    "# labels = [1, 0, 0, 1]  # 1: Positive sentiment, 0: Negative sentiment\n",
    "X_train = df_image_train['text']\n",
    "y_train = df_image_train['label']\n",
    "\n",
    "X_val = df_image_val['text']\n",
    "y_val = df_image_val['label']\n",
    "\n",
    "# X_test = df_image_test['text']\n",
    "# y_test = df_image_test['label']\n",
    "\n",
    "# # Split into training and testing sets\n",
    "# train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2)\n",
    "\n",
    "# Convert the texts to embeddings\n",
    "train_embeddings = model.encode(X_train)\n",
    "val_embeddings = model.encode(X_val)\n",
    "\n",
    "# Convert everything into torch tensors\n",
    "train_embeddings = torch.tensor(train_embeddings)\n",
    "train_labels = torch.tensor(y_train)\n",
    "\n",
    "val_embeddings = torch.tensor(val_embeddings)\n",
    "val_labels = torch.tensor(y_val)\n",
    "\n",
    "# A simple linear classifier on top of SBERT's embeddings\n",
    "classifier = nn.Linear(train_embeddings.size(1), 2)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(classifier.parameters(), lr=1e-2)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(300):  # for simplicity, we train for 10 epochs\n",
    "    classifier.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = classifier(train_embeddings)\n",
    "    loss = criterion(outputs, train_labels)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n",
    "\n",
    "# Test the classifier\n",
    "classifier.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = classifier(val_embeddings)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# Compute precision, recall, and AUROC\n",
    "precision = precision_score(val_labels, predicted, average='weighted')\n",
    "recall = recall_score(val_labels, predicted, average='weighted')\n",
    "roc_auc = roc_auc_score(val_labels, predicted)\n",
    "\n",
    "print('Accuracy: ', (predicted == val_labels).sum().item() / val_labels.size(0))\n",
    "print('Precision: ', precision)\n",
    "print('Recall: ', recall)\n",
    "print('AUROC: ', roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16a96d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"sentence_transformer_final.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16bc4f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.55\n",
      "Precision:  0.56677558910162\n",
      "Recall:  0.55\n",
      "AUROC:  0.5466947240402619\n"
     ]
    }
   ],
   "source": [
    "# Test the classifier\n",
    "classifier.eval()\n",
    "\n",
    "df_image_test = pd.read_json(\"data/dev_seen.jsonl\", lines=True)\n",
    "\n",
    "X_test = df_image_test['text']\n",
    "y_test = df_image_test['label']\n",
    "\n",
    "test_embeddings = model.encode(X_test)\n",
    "\n",
    "test_embeddings = torch.tensor(test_embeddings)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = classifier(test_embeddings)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# Compute precision, recall, and AUROC\n",
    "precision = precision_score(test_labels, predicted, average='weighted')\n",
    "recall = recall_score(test_labels, predicted, average='weighted')\n",
    "roc_auc = roc_auc_score(test_labels, predicted)\n",
    "\n",
    "print('Accuracy: ', (predicted == test_labels).sum().item() / test_labels.size(0))\n",
    "print('Precision: ', precision)\n",
    "print('Recall: ', recall)\n",
    "print('AUROC: ', roc_auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
